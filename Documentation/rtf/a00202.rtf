{\rtf1\ansi\ansicpg1252\uc1 \deff0\deflang1033\deflangfe1033
{\fonttbl {\f0\froman\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}
{\f1\fswiss\fcharset0\fprq2{\*\panose 020b0604020202020204}Arial;}
{\f2\fmodern\fcharset0\fprq1{\*\panose 02070309020205020404}Courier New;}
{\f3\froman\fcharset2\fprq2{\*\panose 05050102010706020507}Symbol;}
}
{\colortbl;\red0\green0\blue0;\red0\green0\blue255;\red0\green255\blue255;\red0\green255\blue0;\red255\green0\blue255;\red255\green0\blue0;\red255\green255\blue0;\red255\green255\blue255;\red0\green0\blue128;\red0\green128\blue128;\red0\green128\blue0;\red128\green0\blue128;\red128\green0\blue0;\red128\green128\blue0;\red128\green128\blue128;\red192\green192\blue192;\red0\green128\blue0;\red96\green64\blue32;\rede0\green128\blue0;\red128\green0\blue0;\red128\green96\blue32;\red0\green32\blue128;\red0\green128\blue128;\red255\green0\blue255;\red0\green0\blue0;\red112\green0\blue112;\red255\green0\blue0;}
{\stylesheet
{\widctlpar\adjustright \fs20\cgrid \snext0 Normal;}
{\paperw11900\paperh16840\margl1800\margr1800\margt1440\margb1440\gutter0\ltrsect}
{\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid \sbasedon0 \snext0 heading 1;}
{\s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid \sbasedon0 \snext0 heading 2;}
{\s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid \sbasedon0 \snext0 heading 3;}
{\s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid \sbasedon0 \snext0 heading 4;}{\*\cs10 \additive Default Paragraph Font;}
{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid \sbasedon0 \snext0 heading 5;}{\*\cs10 \additive Default Paragraph Font;}
{\s15\qc\sb240\sa60\widctlpar\outlinelevel0\adjustright \b\f1\fs32\kerning28\cgrid \sbasedon0 \snext15 Title;}
{\s16\qc\sa60\widctlpar\outlinelevel1\adjustright \f1\cgrid \sbasedon0 \snext16 Subtitle;}
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid \sbasedon0 \snext17 BodyText;}
{\s18\widctlpar\fs22\cgrid \sbasedon0 \snext18 DenseText;}
{\s28\widctlpar\tqc\tx4320\tqr\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext28 header;}
{\s29\widctlpar\tqc\tx4320\tqr\tx8640\qr\adjustright \fs20\cgrid \sbasedon0 \snext29 footer;}
{\s30\li360\sa60\sb120\keepn\widctlpar\adjustright \b\f1\fs20\cgrid \sbasedon0 \snext30 GroupHeader;}
{\s40\li0\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext41 Code Example 0;}
{\s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext42 Code Example 1;}
{\s42\li720\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext43 Code Example 2;}
{\s43\li1080\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext44 Code Example 3;}
{\s44\li1440\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext45 Code Example 4;}
{\s45\li1800\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext46 Code Example 5;}
{\s46\li2160\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext47 Code Example 6;}
{\s47\li2520\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext48 Code Example 7;}
{\s48\li2880\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext49 Code Example 8;}
{\s49\li3240\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext50 Code Example 9;}
{\s50\li3600\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext51 Code Example 10;}
{\s51\li3960\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext52 Code Example 11;}
{\s52\li4320\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext53 Code Example 12;}
{\s53\li4680\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext53 Code Example 13;}
{\s60\li0\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext61 List Continue 0;}
{\s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext62 List Continue 1;}
{\s62\li720\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext63 List Continue 2;}
{\s63\li1080\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext64 List Continue 3;}
{\s64\li1440\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext65 List Continue 4;}
{\s65\li1800\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext66 List Continue 5;}
{\s66\li2160\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext67 List Continue 6;}
{\s67\li2520\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext68 List Continue 7;}
{\s68\li2880\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext69 List Continue 8;}
{\s69\li3240\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext70 List Continue 9;}
{\s70\li3600\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext71 List Continue 10;}
{\s71\li3960\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext72 List Continue 11;}
{\s72\li4320\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext73 List Continue 12;}
{\s73\li4680\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext73 List Continue 13;}
{\s80\li0\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext81 DescContinue 0;}
{\s81\li360\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext82 DescContinue 1;}
{\s82\li720\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext83 DescContinue 2;}
{\s83\li1080\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext84 DescContinue 3;}
{\s84\li1440\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext85 DescContinue 4;}
{\s85\li1800\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext86 DescContinue 5;}
{\s86\li2160\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext87 DescContinue 6;}
{\s87\li2520\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext88 DescContinue 7;}
{\s88\li2880\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext89 DescContinue 8;}
{\s89\li3240\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext90 DescContinue 9;}
{\s90\li3600\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext91 DescContinue 10;}
{\s91\li3960\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext92 DescContinue 11;}
{\s92\li4320\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext93 DescContinue 12;}
{\s93\li4680\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext93 DescContinue 13;}
{\s100\li0\sa30\sb30\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext101 LatexTOC 0;}
{\s101\li360\sa27\sb27\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext102 LatexTOC 1;}
{\s102\li720\sa24\sb24\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext103 LatexTOC 2;}
{\s103\li1080\sa21\sb21\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext104 LatexTOC 3;}
{\s104\li1440\sa18\sb18\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext105 LatexTOC 4;}
{\s105\li1800\sa15\sb15\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext106 LatexTOC 5;}
{\s106\li2160\sa12\sb12\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext107 LatexTOC 6;}
{\s107\li2520\sa9\sb9\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext108 LatexTOC 7;}
{\s108\li2880\sa6\sb6\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext109 LatexTOC 8;}
{\s109\li3240\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext110 LatexTOC 9;}
{\s110\li3600\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext111 LatexTOC 10;}
{\s111\li3960\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext112 LatexTOC 11;}
{\s112\li4320\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext113 LatexTOC 12;}
{\s113\li4680\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext113 LatexTOC 13;}
{\s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext121 \sautoupd List Bullet 0;}
{\s121\fi-360\li720\widctlpar\jclisttab\tx720{\*\pn \pnlvlbody\ilvl0\ls2\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext122 \sautoupd List Bullet 1;}
{\s122\fi-360\li1080\widctlpar\jclisttab\tx1080{\*\pn \pnlvlbody\ilvl0\ls3\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext123 \sautoupd List Bullet 2;}
{\s123\fi-360\li1440\widctlpar\jclisttab\tx1440{\*\pn \pnlvlbody\ilvl0\ls4\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext124 \sautoupd List Bullet 3;}
{\s124\fi-360\li1800\widctlpar\jclisttab\tx1800{\*\pn \pnlvlbody\ilvl0\ls5\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext125 \sautoupd List Bullet 4;}
{\s125\fi-360\li2160\widctlpar\jclisttab\tx2160{\*\pn \pnlvlbody\ilvl0\ls6\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext126 \sautoupd List Bullet 5;}
{\s126\fi-360\li2520\widctlpar\jclisttab\tx2520{\*\pn \pnlvlbody\ilvl0\ls7\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext127 \sautoupd List Bullet 6;}
{\s127\fi-360\li2880\widctlpar\jclisttab\tx2880{\*\pn \pnlvlbody\ilvl0\ls8\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext128 \sautoupd List Bullet 7;}
{\s128\fi-360\li3240\widctlpar\jclisttab\tx3240{\*\pn \pnlvlbody\ilvl0\ls9\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext129 \sautoupd List Bullet 8;}
{\s129\fi-360\li3600\widctlpar\jclisttab\tx3600{\*\pn \pnlvlbody\ilvl0\ls10\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext130 \sautoupd List Bullet 9;}
{\s130\fi-360\li3960\widctlpar\jclisttab\tx3960{\*\pn \pnlvlbody\ilvl0\ls11\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext131 \sautoupd List Bullet 10;}
{\s131\fi-360\li4320\widctlpar\jclisttab\tx4320{\*\pn \pnlvlbody\ilvl0\ls12\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext132 \sautoupd List Bullet 11;}
{\s132\fi-360\li4680\widctlpar\jclisttab\tx4680{\*\pn \pnlvlbody\ilvl0\ls13\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext133 \sautoupd List Bullet 12;}
{\s133\fi-360\li5040\widctlpar\jclisttab\tx5040{\*\pn \pnlvlbody\ilvl0\ls14\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext133 \sautoupd List Bullet 13;}
{\s140\fi-360\li360\widctlpar\fs20\cgrid \sbasedon0 \snext141 \sautoupd List Enum 0;}
{\s141\fi-360\li720\widctlpar\fs20\cgrid \sbasedon0 \snext142 \sautoupd List Enum 1;}
{\s142\fi-360\li1080\widctlpar\fs20\cgrid \sbasedon0 \snext143 \sautoupd List Enum 2;}
{\s143\fi-360\li1440\widctlpar\fs20\cgrid \sbasedon0 \snext144 \sautoupd List Enum 3;}
{\s144\fi-360\li1800\widctlpar\fs20\cgrid \sbasedon0 \snext145 \sautoupd List Enum 4;}
{\s145\fi-360\li2160\widctlpar\fs20\cgrid \sbasedon0 \snext146 \sautoupd List Enum 5;}
{\s146\fi-360\li2520\widctlpar\fs20\cgrid \sbasedon0 \snext147 \sautoupd List Enum 6;}
{\s147\fi-360\li2880\widctlpar\fs20\cgrid \sbasedon0 \snext148 \sautoupd List Enum 7;}
{\s148\fi-360\li3240\widctlpar\fs20\cgrid \sbasedon0 \snext149 \sautoupd List Enum 8;}
{\s149\fi-360\li3600\widctlpar\fs20\cgrid \sbasedon0 \snext150 \sautoupd List Enum 9;}
{\s150\fi-360\li3960\widctlpar\fs20\cgrid \sbasedon0 \snext151 \sautoupd List Enum 10;}
{\s151\fi-360\li4320\widctlpar\fs20\cgrid \sbasedon0 \snext152 \sautoupd List Enum 11;}
{\s152\fi-360\li4680\widctlpar\fs20\cgrid \sbasedon0 \snext153 \sautoupd List Enum 12;}
{\s153\fi-360\li5040\widctlpar\fs20\cgrid \sbasedon0 \snext153 \sautoupd List Enum 13;}
}
{\comment begin body}
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
room_estimation\par \pard\plain 
{\tc\tcl2 \v room_estimation}
{\xe \v room_estimation}
{\bkmkstart AAAAAAAAPT}
{\bkmkend AAAAAAAAPT}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Public Member Functions\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAPU" }{}}{\fldrslt {\cs37\ul\cf2 __init__}}}
 (self, image=None, camera_matrix=JUSTIN_HERO4_SILVER_720_MEDIUM_FOV_MTX, distortion_matrix=JUSTIN_HERO4_SILVER_720_MEDIUM_FOV_DIST, rvecs=JUSTIN_HERO4_SILVER_720_MEDIUM_FOV_RVEC)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAPV" }{}}{\fldrslt {\cs37\ul\cf2 set_image}}}
 (self, image)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAPW" }{}}{\fldrslt {\cs37\ul\cf2 display_room}}}
 (self, image=None, axis=True, box=True, show_3d_plot=True, with_points=False)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAPX" }{}}{\fldrslt {\cs37\ul\cf2 set_axis}}}
 (self, x, y, z)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAPY" }{}}{\fldrslt {\cs37\ul\cf2 set_corners}}}
 (self, p1=None, p2=None, p3=None, p4=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAPZ" }{}}{\fldrslt {\cs37\ul\cf2 img_copy}}}
 (self)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQA" }{}}{\fldrslt {\cs37\ul\cf2 define_sides}}}
 (self, sides, lengths)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQB" }{}}{\fldrslt {\cs37\ul\cf2 define_room}}}
 (self, width=None, length=None, height=None, offset=[0, 0, 0, 0], corners=None, room_points=None, refine_corners=False, image=None, stitch_videos=None, calibration=None, extend=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQC" }{}}{\fldrslt {\cs37\ul\cf2 clear_corners}}}
 (self)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQD" }{}}{\fldrslt {\cs37\ul\cf2 get_room_corners}}}
 (self, num_points=4)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQE" }{}}{\fldrslt {\cs37\ul\cf2 edit_room_corner}}}
 (self, index, new_pixel)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQF" }{}}{\fldrslt {\cs37\ul\cf2 connect_points}}}
 (self, points)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQG" }{}}{\fldrslt {\cs37\ul\cf2 get_corners}}}
 (self, event, x, y, flags, param)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQH" }{}}{\fldrslt {\cs37\ul\cf2 get_corner_with_definition}}}
 (self, event, x, y, flags, param)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQI" }{}}{\fldrslt {\cs37\ul\cf2 get_cube_side}}}
 (self, cube, side)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQJ" }{}}{\fldrslt {\cs37\ul\cf2 crop_plain}}}
 (self, points, plain_width, plain_length, image=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQK" }{}}{\fldrslt {\cs37\ul\cf2 project_points}}}
 (self, points=[])\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQL" }{}}{\fldrslt {\cs37\ul\cf2 plot_points}}}
 (event, x, y, flags, params)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQM" }{}}{\fldrslt {\cs37\ul\cf2 get_3d_to_2d}}}
 (self, point, show=True)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQN" }{}}{\fldrslt {\cs37\ul\cf2 get_3d_point}}}
 (self, pixel, height=1500, camera_matrix=None, rotation_matrix=None, tvec=None, show_point=True)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQO" }{}}{\fldrslt {\cs37\ul\cf2 display_3D_Plot}}}
 (points, shown)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQP" }{}}{\fldrslt {\cs37\ul\cf2 draw_axis}}}
 (self)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQQ" }{}}{\fldrslt {\cs37\ul\cf2 draw_cuboid}}}
 (self, image=None, distortion=None, rvec=None, tvec=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQR" }{}}{\fldrslt {\cs37\ul\cf2 draw_box}}}
 (img, corners, imgpts)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQS" }{}}{\fldrslt {\cs37\ul\cf2 draw_point}}}
 (self, img, front_pt, back_pt)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQT" }{}}{\fldrslt {\cs37\ul\cf2 estimate_plane}}}
 (img, corners, points, calibration_matrix, distortion_matrix)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQU" }{}}{\fldrslt {\cs37\ul\cf2 draw_vector}}}
 (self, event, x, y, flags, params)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQV" }{}}{\fldrslt {\cs37\ul\cf2 get_room_3d}}}
 (self, limits, height, step, save_filename=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQW" }{}}{\fldrslt {\cs37\ul\cf2 save_room_3d}}}
 (self, path, filename, mapped)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQX" }{}}{\fldrslt {\cs37\ul\cf2 load_room_3d}}}
 (self, filename)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQY" }{}}{\fldrslt {\cs37\ul\cf2 get_room_points}}}
 (self, width, length, height=0)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQZ" }{}}{\fldrslt {\cs37\ul\cf2 get_n_room_points}}}
 (self, n_locations=None, height=0)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARA" }{}}{\fldrslt {\cs37\ul\cf2 assign_non_square_corners}}}
 (self, num_points=inf)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARB" }{}}{\fldrslt {\cs37\ul\cf2 find_3d_limits}}}
 (self, height=1000)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARC" }{}}{\fldrslt {\cs37\ul\cf2 project_point}}}
 (self, event, x, y, flags, params)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARD" }{}}{\fldrslt {\cs37\ul\cf2 get_depth}}}
 (self, event, x, y, p1, p2)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARE" }{}}{\fldrslt {\cs37\ul\cf2 estimate}}}
 (self)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARF" }{}}{\fldrslt {\cs37\ul\cf2 get_pixel_location}}}
 (self, pixel)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARG" }{}}{\fldrslt {\cs37\ul\cf2 poly_fit_wall}}}
 (self, samples, pixels_past_extent=100, horizontal_extent=None, degree=2)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARH" }{}}{\fldrslt {\cs37\ul\cf2 find_intersection}}}
 (self, line_equation1, line_equation2)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARI" }{}}{\fldrslt {\cs37\ul\cf2 make_interpolater}}}
 (left_min, left_max, right_min, right_max)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARJ" }{}}{\fldrslt {\cs37\ul\cf2 superimpose_checker}}}
 (self, size=21)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARK" }{}}{\fldrslt {\cs37\ul\cf2 define_perspective_grid}}}
 (self, division=10, extend=500)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARL" }{}}{\fldrslt {\cs37\ul\cf2 find_wall_intersection}}}
 (self, number_of_walls=2, samples_per_wall=4, extent=10000, intersection_pairs=[], degree=1)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARM" }{}}{\fldrslt {\cs37\ul\cf2 get_checkerboard_shape}}}
 (self, checkerboard_images)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARN" }{}}{\fldrslt {\cs37\ul\cf2 fisheye_calibrate}}}
 (self, checkerboard_images, checkerboard_grid=None, out=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARO" }{}}{\fldrslt {\cs37\ul\cf2 checkerboard_calibrate}}}
 (self, checkerboard_images, checkerboard_grid=None, out=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARP" }{}}{\fldrslt {\cs37\ul\cf2 save_calibration}}}
 (self, file, mtx, distortion, rvecs, tvecs, error_list, resolution)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARQ" }{}}{\fldrslt {\cs37\ul\cf2 load_calibration}}}
 (self, file)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARR" }{}}{\fldrslt {\cs37\ul\cf2 collect_frames}}}
 (self, video_source, start_frame=1, skip=15, total_frames=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARS" }{}}{\fldrslt {\cs37\ul\cf2 get_calibration_ratio}}}
 (self, video_resolution, calibration_resolution)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAART" }{}}{\fldrslt {\cs37\ul\cf2 undistort_room}}}
 (self, image=None, points_data=None, camera_matrix=None, distortion=None, show=False, fisheye=False, calibration_resolution=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARU" }{}}{\fldrslt {\cs37\ul\cf2 undistort_tracker_data}}}
 (self, data, image=None, camera_matrix=None, distortion=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARV" }{}}{\fldrslt {\cs37\ul\cf2 load_tracker}}}
 (self, csv_file)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARW" }{}}{\fldrslt {\cs37\ul\cf2 correct_tracker_points}}}
 (self, df)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARX" }{}}{\fldrslt {\cs37\ul\cf2 update_tracker_3D}}}
 (self, index, df, mapped_min, mapped_max)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARY" }{}}{\fldrslt {\cs37\ul\cf2 show_tracker_3D}}}
 (self, df, mapped_min, mapped_max)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARZ" }{}}{\fldrslt {\cs37\ul\cf2 show_tracker_2D}}}
 (self, df, image, data_columns=['Pixel_Loc_x', 'Pixel_Loc_y'], window="Tracker2D", colour=(255, 255, 0))\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASA" }{}}{\fldrslt {\cs37\ul\cf2 extend_image_to_corners}}}
 (self, image, corners)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASB" }{}}{\fldrslt {\cs37\ul\cf2 get_room_dimensions}}}
 (self, room_points)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASC" }{}}{\fldrslt {\cs37\ul\cf2 evaluate_calibration}}}
 (self)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASD" }{}}{\fldrslt {\cs37\ul\cf2 visualize_distortion}}}
 (self, camera_matrix, distiortion, image=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABQ" }{}}{\fldrslt {\cs37\ul\cf2 save_room}}}
 (self, filename)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASE" }{}}{\fldrslt {\cs37\ul\cf2 load_room}}}
 (self, filename)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASF" }{}}{\fldrslt {\cs37\ul\cf2 load_and_undistort_calibration}}}
 (self, video_path, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASD" }{}}{\fldrslt {\cs37\ul\cf2 visualize_distortion}}}
=False, points_data=None)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABR" }{}}{\fldrslt {\cs37\ul\cf2 stitch_rooms}}}
 (self, video_list, undistort=True, key_index=None, show=False)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABS" }{}}{\fldrslt {\cs37\ul\cf2 stitch_trackers}}}
 (self, video_list, homography_list, use_csv=True, offsets=[0, 0, 0, 0])\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASG" }{}}{\fldrslt {\cs37\ul\cf2 draw_grid}}}
 (self, grid_shape=(20, 20), color=(255, 255, 255), thickness=2)\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASH" }{}}{\fldrslt {\cs37\ul\cf2 get_files_from_folder}}}
 (self, folder, extension="MP4")\par
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Public Attributes\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b window_name}{\bkmkstart AAAAAAAASI}
{\bkmkend AAAAAAAASI}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b image}{\bkmkstart AAAAAAAASJ}
{\bkmkend AAAAAAAASJ}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b show_img}{\bkmkstart AAAAAAAASK}
{\bkmkend AAAAAAAASK}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b corners}{\bkmkstart AAAAAAAASL}
{\bkmkend AAAAAAAASL}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b room_points}{\bkmkstart AAAAAAAASM}
{\bkmkend AAAAAAAASM}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b real_locations}{\bkmkstart AAAAAAAASN}
{\bkmkend AAAAAAAASN}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b camera_matrix}{\bkmkstart AAAAAAAASO}
{\bkmkend AAAAAAAASO}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b distioriton_matrix}{\bkmkstart AAAAAAAASP}
{\bkmkend AAAAAAAASP}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b calibration_errors}{\bkmkstart AAAAAAAASQ}
{\bkmkend AAAAAAAASQ}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b calibration_resolution}{\bkmkstart AAAAAAAASR}
{\bkmkend AAAAAAAASR}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b rotation_vector}{\bkmkstart AAAAAAAASS}
{\bkmkend AAAAAAAASS}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b translation_vector}{\bkmkstart AAAAAAAAST}
{\bkmkend AAAAAAAAST}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b room_width}{\bkmkstart AAAAAAAASU}
{\bkmkend AAAAAAAASU}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b room_length}{\bkmkstart AAAAAAAASV}
{\bkmkend AAAAAAAASV}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b room_height}{\bkmkstart AAAAAAAASW}
{\bkmkend AAAAAAAASW}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b sample_rate}{\bkmkstart AAAAAAAASX}
{\bkmkend AAAAAAAASX}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b mapped_dictionary}{\bkmkstart AAAAAAAASY}
{\bkmkend AAAAAAAASY}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b mapped}{\bkmkstart AAAAAAAASZ}
{\bkmkend AAAAAAAASZ}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b points_data}{\bkmkstart AAAAAAAATA}
{\bkmkend AAAAAAAATA}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b vector_depth}{\bkmkstart AAAAAAAATB}
{\bkmkend AAAAAAAATB}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b axis}{\bkmkstart AAAAAAAATC}
{\bkmkend AAAAAAAATC}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b fig}{\bkmkstart AAAAAAAATD}
{\bkmkend AAAAAAAATD}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b stitch_homographies}{\bkmkstart AAAAAAAATE}
{\bkmkend AAAAAAAATE}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b show_3D_plot}{\bkmkstart AAAAAAAATF}
{\bkmkend AAAAAAAATF}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b get_corners}{\bkmkstart AAAAAAAATG}
{\bkmkend AAAAAAAATG}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b ax}{\bkmkstart AAAAAAAATH}
{\bkmkend AAAAAAAATH}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b front_plt}{\bkmkstart AAAAAAAATI}
{\bkmkend AAAAAAAATI}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b back_plt}{\bkmkstart AAAAAAAATJ}
{\bkmkend AAAAAAAATJ}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b float_plt}{\bkmkstart AAAAAAAATK}
{\bkmkend AAAAAAAATK}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b line_plt}{\bkmkstart AAAAAAAATL}
{\bkmkend AAAAAAAATL}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b points_plt}{\bkmkstart AAAAAAAATM}
{\bkmkend AAAAAAAATM}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b quiver}{\bkmkstart AAAAAAAATN}
{\bkmkend AAAAAAAATN}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b get_corner_with_definition}{\bkmkstart AAAAAAAATO}
{\bkmkend AAAAAAAATO}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b get_depth}{\bkmkstart AAAAAAAATP}
{\bkmkend AAAAAAAATP}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b error_list}{\bkmkstart AAAAAAAATQ}
{\bkmkend AAAAAAAATQ}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b invert_y}{\bkmkstart AAAAAAAATR}
{\bkmkend AAAAAAAATR}
\par
}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Constructor & Destructor Documentation\par
\pard\plain 
{\xe \v __init__\:room_estimation}
{\xe \v room_estimation\:__init__}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
__init__ (  {\i self},   {\i image} = {\f2 None},   {\i camera_matrix} = {\f2 JUSTIN_HERO4_SILVER_720_MEDIUM_FOV_MTX},   {\i distortion_matrix} = {\f2 JUSTIN_HERO4_SILVER_720_MEDIUM_FOV_DIST},   {\i rvecs} = {\f2 JUSTIN_HERO4_SILVER_720_MEDIUM_FOV_RVEC})}}
\par
{\bkmkstart AAAAAAAAPU}
{\bkmkend AAAAAAAAPU}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 148     {\cf17 def }__init__(self, image=None, camera_matrix=JUSTIN_HERO4_SILVER_720_MEDIUM_FOV_MTX, distortion_matrix=JUSTIN_HERO4_SILVER_720_MEDIUM_FOV_DIST, rvecs=JUSTIN_HERO4_SILVER_720_MEDIUM_FOV_RVEC):\par
149 \par
150         \par
151         self.window_name = {\cf22 "Room Estimation"}\par
152         cv2.namedWindow({\cf22 "Room Estimation"})\par
153         self.image = image\par
154         self.show_img = self.img_copy()\par
155         self.corners = []\par
156         self.room_points = [] {\cf20 # The formatted real_locations that is used in processing data}\par
157 \par
158         self.real_locations = [] {\cf20 # Raw input of points in 3D [(0,0,0), (1000,0 ,0), (1000, 2000, 0), (0, 2000,0)]}\par
159 \par
160         self.camera_matrix = camera_matrix\par
161         self.distioriton_matrix = distortion_matrix\par
162         self.calibration_errors = {\cf18 None}\par
163 \par
164         self.calibration_resolution = {\cf18 None}\par
165 \par
166         self.rotation_vector = rvecs\par
167         self.translation_vector = {\cf18 None}\par
168 \par
169         {\cf20 # Size are in millimeters}\par
170         self.room_width = 3000  {\cf20 # X}\par
171         self.room_length = 3000 {\cf20 # Depth (Z)}\par
172         self.room_height = 3000 {\cf20 # Y}\par
173 \par
174         self.sample_rate = 5   {\cf20 # estimates position every 1cm}\par
175 \par
176         self.mapped_dictionary = \{\}\par
177         self.mapped = \{\}\par
178 \par
179         self.points_data = {\cf18 None}\par
180 \par
181         self.vector_depth = 0\par
182 \par
183         self.axis = self.set_axis(3000,1000,3000)\par
184         self.fig = {\cf18 None}\par
185         self.stitch_homographies = {\cf18 None}\par
186 \par
187         {\cf20 # img = self.draw_axis()}\par
188         {\cf20 # cv2.waitKey(1)}\par
189 \par
190         {\cf20 # self.mapped_dictionary = self.map_2d_to_3d((self.room_width, self.room_length, self.room_height))}\par
191         \par
192         self.show_3D_plot = {\cf17 True}\par
193         {\cf20 # while True:}\par
194         {\cf20 #     cv2.imshow(self.window_name, self.show_img)}\par
195         {\cf20 #     cv2.waitKey(1)}\par
196         {\cf20 # self.corner_points = []}\par
197 \par
}
}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Member Function Documentation\par
\pard\plain 
{\xe \v assign_non_square_corners\:room_estimation}
{\xe \v room_estimation\:assign_non_square_corners}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
assign_non_square_corners (  {\i self},   {\i num_points} = {\f2 inf})}}
\par
{\bkmkstart AAAAAAAARA}
{\bkmkend AAAAAAAARA}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Assigns points around the room with any number of known coordinates.\par
\par
Assign point\par
Set width and length of known coordinate.\par
Origin should be 0,0\par
Sets the dimentions of the room to the largest dimensions\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1109     {\cf17 def }assign_non_square_corners(self, num_points=inf):\par
1110         {\cf22 '''}\par
1111 {\cf22         Assigns points around the room with any number of known coordinates.}\par
1112 {\cf22 }\par
1113 {\cf22         Assign point}\par
1114 {\cf22         Set width and length of known coordinate.}\par
1115 {\cf22         Origin should be 0,0}\par
1116 {\cf22         Sets the dimentions of the room to the largest dimensions}\par
1117 {\cf22         '''}\par
1118 \par
1119         print({\cf22 "Assigning corners..."})\par
1120         {\cf20 #Initializes feedback to a function}\par
1121         cv2.setMouseCallback({\cf22 "Room Estimation"}, self.get_corners)\par
1122         {\cf19 while} len(self.corners) < num_points:\par
1123             \par
1124             {\cf20 # if len(self.corners) >= 4:}\par
1125             {\cf20 #     print("SHOWING ROOM")}\par
1126             {\cf20 #     self.show_img, cube, target_cube = self.draw_cuboid()}\par
1127             {\cf20 # print(len(self.corners))}\par
1128 \par
1129             {\cf20 # display the image and wait for a keypress}\par
1130             cv2.setMouseCallback({\cf22 "Room Estimation"}, self.get_corner_with_definition)\par
1131 \par
1132 \par
1133 \par
1134             cv2.imshow({\cf22 "Room Estimation"}, self.show_img)\par
1135             key = cv2.waitKey(1) & 0xFF\par
1136             self.connect_points(self.corners)\par
1137 \par
1138 \par
1139 \par
1140             {\cf19 if} key == ord({\cf22 "c"}):\par
1141                 {\cf19 if} len(self.corners) < 4:\par
1142                     print({\cf22 "You need at least 4 points"})\par
1143                 {\cf19 else}:\par
1144                     {\cf20 # break}\par
1145                     self.corners = np.reshape(np.asfarray(self.corners), (len(self.corners),2,1))\par
1146                     self.room_points = self.get_n_room_points()\par
1147 \par
1148                     {\cf20 # set measurments to maximum sizes}\par
1149                     self.room_width = 0\par
1150                     self.room_length = 0\par
1151                     {\cf19 for} point {\cf19 in} self.room_points:\par
1152                         width = int(point[0][0])\par
1153                         length = int(point[1][0])\par
1154                         {\cf19 if} width > self.room_width:\par
1155                             self.room_width = width\par
1156                         {\cf19 if} length > self.room_length:\par
1157                             self.room_length = length\par
1158 \par
1159                     {\cf19 return} self.room_points, self.corners\par
1160 \par
1161         self.connect_points(self.corners)\par
1162 \par
1163         {\cf20 # set measurments to maximum sizes }\par
1164         {\cf19 for} point {\cf19 in} self.room_points:\par
1165             width = int(point[0][0])\par
1166             length = int(point[1][0])\par
1167             {\cf19 if} width > self.room_width:\par
1168                 self.room_width = width\par
1169             {\cf19 if} length > self.room_length:\par
1170                 self.room_length = length\par
1171         \par
1172         {\cf20 # Set measurments to numpy arrays}\par
1173         self.corners = np.reshape(np.asfarray(self.corners), (len(self.corners),2,1))\par
1174         self.room_points = self.get_n_room_points()\par
1175         {\cf20 # cv2.setMouseCallback("Room Estimation", self.draw_vector)}\par
1176         {\cf19 return} self.room_points, self.corners\par
1177 \par
}
{
\ql
References room_estimation.camera_matrix, room_estimation.connect_points(), room_estimation.corners, room_estimation.distioriton_matrix, room_estimation.get_corner_with_definition(), room_estimation.get_corner_with_definition, room_estimation.get_corners, room_estimation.get_corners(), {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQZ" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.get_n_room_points()}}}
, room_estimation.room_length, room_estimation.room_points, room_estimation.room_width, room_estimation.rotation_vector, room_estimation.show_img, and room_estimation.translation_vector.}\par
}
{\xe \v checkerboard_calibrate\:room_estimation}
{\xe \v room_estimation\:checkerboard_calibrate}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
checkerboard_calibrate (  {\i self},   {\i checkerboard_images},   {\i checkerboard_grid} = {\f2 None},   {\i out} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAARO}
{\bkmkend AAAAAAAARO}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1781     {\cf17 def }checkerboard_calibrate(self, checkerboard_images, checkerboard_grid=None, out=None):\par
1782         {\cf20 # Will look at what this means after}\par
1783         criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\par
1784         {\cf20 # calibration_flags = cv2.fisheye.CALIB_RECOMPUTE_EXTRINSIC+cv2.fisheye.CALIB_CHECK_COND+cv2.fisheye.CALIB_FIX_SKEW}\par
1785         {\cf20 # calibration_flags = cv2.fisheye.CALIB_RECOMPUTE_EXTRINSIC + cv2.fisheye.CALIB_CHECK_COND + cv2.fisheye.CALIB_FIX_SKEW + cv2.}\par
1786 \par
1787         calibration_flags = cv2.CALIB_RATIONAL_MODEL + cv2.CALIB_FIX_PRINCIPAL_POINT\par
1788 \par
1789         {\cf20 # Creating vector to store vectors of 3D points for each checkerboard image}\par
1790         objpoints = []\par
1791         {\cf20 # Creating vector to store vectors of 2D points for each checkerboard image}\par
1792         imgpoints = [] \par
1793 \par
1794         imgshape = {\cf18 None}\par
1795     \par
1796         checkerboard_grid = self.get_checkerboard_shape(checkerboard_images)\par
1797         height, width = checkerboard_images[0].shape[:2]\par
1798 \par
1799         {\cf19 for} image {\cf19 in} checkerboard_images:\par
1800 \par
1801             image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\par
1802 \par
1803             cv2.imshow({\cf22 'img'},image)\par
1804             cv2.waitKey(1)\par
1805             imgshape = image.shape\par
1806             retval, corners = cv2.findChessboardCorners(image, checkerboard_grid)\par
1807             {\cf20 # cv2.cornerSubPix(checkerboard_image, corners)}\par
1808 \par
1809             {\cf20 # retval, cameraMatrix, distCoeffs, rvecs, tvecs = cv2.calibrateCamera(objectPoints, imagePoints, imageSize)}\par
1810             objp = np.zeros((1, checkerboard_grid[0] * checkerboard_grid[1], 3), np.float32)\par
1811             objp[0,:,:2] = np.mgrid[0:checkerboard_grid[0], 0:checkerboard_grid[1]].T.reshape(-1, 2)\par
1812             prev_img_shape = {\cf18 None}\par
1813      \par
1814 \par
1815 \par
1816             {\cf19 if} retval == {\cf17 True}:\par
1817                 objpoints.append(objp)\par
1818                 {\cf20 # refining pixel coordinates for given 2d points.}\par
1819                 corners2 = cv2.cornerSubPix(image, corners, (11,11),(-1,-1), criteria)  \par
1820                 imgpoints.append(corners2)\par
1821                 {\cf20 # Draw and display the corners}\par
1822 \par
1823                 image = cv2.drawChessboardCorners(image, checkerboard_grid, corners2, retval)\par
1824                 cv2.imshow({\cf22 "Calibration"}, image)\par
1825                 cv2.waitKey(1)\par
1826         cv2.destroyAllWindows()\par
1827 \par
1828 \par
1829 \par
1830         {\cf20 # cv2.calibrateCameraExtended()}\par
1831         print({\cf22 "Calibrating... "}, len(objpoints), {\cf22 " images"})\par
1832         ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, imgshape[::-1], {\cf18 None}, {\cf18 None}, flags=calibration_flags, criteria=criteria)\par
1833         \par
1834         mean_error = 0\par
1835         total_error = 0\par
1836         error_list = []\par
1837         {\cf19 for} i {\cf19 in} range(len(objpoints)):\par
1838             imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)\par
1839             error = cv2.norm(imgpoints[i],imgpoints2, cv2.NORM_L2)/len(imgpoints2)\par
1840             total_error += error\par
1841             error_list.append(error)\par
1842         mean_error = total_error/len(objpoints)\par
1843 \par
1844         print(mean_error)\par
1845         self.camera_matrix = mtx\par
1846         self.distioriton_matrix = dist\par
1847         self.rotation_vector = rvecs\par
1848         self.translation_vector = tvecs\par
1849         self.error_list = error_list\par
1850         self.calibration_resolution = (width, height)\par
1851         self.save_calibration(out, mtx, dist, rvecs, tvecs, error_list, self.calibration_resolution)\par
1852         print({\cf22 "Camera:"} ,mtx, {\cf22 "Distoriton:"}, dist)\par
1853         {\cf19 return} mtx, dist, rvecs, tvecs, mean_error\par
1854 \par
}
}
{\xe \v clear_corners\:room_estimation}
{\xe \v room_estimation\:clear_corners}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
clear_corners (  {\i self})}}
\par
{\bkmkstart AAAAAAAAQC}
{\bkmkend AAAAAAAAQC}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 462     {\cf17 def }clear_corners(self):\par
463         print({\cf22 "Clearing Corners!"})\par
464         self.corners = []\par
465         {\cf19 return} self.corners\par
466     \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARK" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.define_perspective_grid()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASA" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.extend_image_to_corners()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARL" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.find_wall_intersection()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQD" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.get_room_corners()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARJ" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.superimpose_checker()}}}
.}\par
}
{\xe \v collect_frames\:room_estimation}
{\xe \v room_estimation\:collect_frames}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
collect_frames (  {\i self},   {\i video_source},   {\i start_frame} = {\f2 1},   {\i skip} = {\f2 15},   {\i total_frames} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAARR}
{\bkmkend AAAAAAAARR}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Collects the images for stitching\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1893     {\cf17 def }collect_frames(self, video_source, start_frame=1, skip=15, total_frames=None):\par
1894         {\cf22 """}\par
1895 {\cf22         Collects the images for stitching}\par
1896 {\cf22         """}\par
1897         print(video_source)\par
1898         cap = cv2.VideoCapture(video_source)\par
1899         \par
1900         {\cf19 if} {\cf19 not} cap.isOpened():\par
1901             {\cf19 raise} ValueError({\cf22 "Unable to open video source"}, video_source)\par
1902         print({\cf22 "Collecting Frames..."})\par
1903         {\cf19 if} total_frames == {\cf18 None}:\par
1904             total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\par
1905         {\cf20 #setup cv2 capture from video}\par
1906         cap = cv2.VideoCapture(video_source)\par
1907         frames = []\par
1908         {\cf20 #set the starting frame}\par
1909         cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\par
1910         {\cf20 # print("Starting at frame: " + str(cap.get(cv2.CAP_PROP_POS_FRAMES)))}\par
1911 \par
1912         last = 0\par
1913         {\cf19 while} (cap.get(cv2.CAP_PROP_POS_FRAMES)+ skip-1) < total_frames:\par
1914             print(cap.get(cv2.CAP_PROP_POS_FRAMES), {\cf22 "/"}, total_frames)\par
1915             last = cap.get(cv2.CAP_PROP_POS_FRAMES)\par
1916             {\cf20 # print(frames)}\par
1917             {\cf20 #read the image from that skipped frame}\par
1918             ret, frame = cap.read()\par
1919 \par
1920             cap.set(cv2.CAP_PROP_POS_FRAMES, cap.get(cv2.CAP_PROP_POS_FRAMES) + skip-1)\par
1921 \par
1922             {\cf19 if} last == cap.get(cv2.CAP_PROP_POS_FRAMES):\par
1923                 {\cf20 # print("NEXT")}\par
1924                 next=10\par
1925                 last = cap.get(cv2.CAP_PROP_POS_FRAMES)\par
1926                 {\cf19 while} last == cap.get(cv2.CAP_PROP_POS_FRAMES):\par
1927                     next += 10\par
1928                     print(next)\par
1929                     ret, frame = cap.read()\par
1930                     cap.set(cv2.CAP_PROP_POS_FRAMES, cap.get(cv2.CAP_PROP_POS_FRAMES) + next + skip-1)\par
1931             {\cf20 #set current frame to the next n-skipped frames}\par
1932             {\cf20 # print(cap.get(cv2.CAP_PROP_POS_FRAMES))}\par
1933 \par
1934             {\cf19 if} ret:\par
1935                 {\cf20 # print(ret)}\par
1936                 {\cf20 # if cv2.waitKey(30) & 0xFF == ord('q'):}\par
1937                 {\cf20 #     break}\par
1938                 {\cf20 # cv2.imshow('frame', frame)}\par
1939                 {\cf20 #append the frames to be processed}\par
1940                 frames.append(frame)\par
1941 \par
1942                 cv2.imshow({\cf22 "FrameCollection"}, frame)\par
1943                 cv2.waitKey(1)\par
1944 \par
1945         cv2.destroyAllWindows()\par
1946         {\cf19 return} frames\par
1947 \par
}
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABQ" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.save_room()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABR" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.stitch_rooms()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABS" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.stitch_trackers()}}}
.}\par
}
{\xe \v connect_points\:room_estimation}
{\xe \v room_estimation\:connect_points}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
connect_points (  {\i self},   {\i points})}}
\par
{\bkmkstart AAAAAAAAQF}
{\bkmkend AAAAAAAAQF}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 506     {\cf17 def }connect_points(self, points):\par
507         {\cf22 "Helper functio nthat visualizes corners"}\par
508         {\cf19 for} index, point {\cf19 in} enumerate(points):\par
509             {\cf19 if} self.room_points:\par
510                 p1 = (int(points[index][0]), int(points[index][1]))\par
511                 self.show_img = cv2.putText(self.show_img, str(self.room_points[index]), p1, fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=1, color=(255,0,0), thickness=2)\par
512             {\cf20 # if index+1 == len(points):}\par
513             {\cf20 #     # print("Last point")}\par
514             {\cf20 #     p1 = (int(points[index][0]), int(points[index][1]))}\par
515             {\cf20 #     p2 = (int(points[0][0]), int(points[0][1]))}\par
516             {\cf20 #     cv2.line(self.show_img, p1, p2, (255,0,0))}\par
517             {\cf19 if} len(points) > 1 {\cf19 and} index+1 < len(points):\par
518                 p1 = (int(points[index][0]), int(points[index][1]))\par
519                 p2 = (int(points[index+1][0]), int(points[index+1][1]))\par
520                 self.show_img=cv2.line(self.show_img, p1, p2, (255,0,0))\par
521                 \par
522 \par
523         cv2.imshow(self.window_name, self.show_img)\par
524 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARA" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.assign_non_square_corners()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARL" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.find_wall_intersection()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQD" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.get_room_corners()}}}
.}\par
}
{\xe \v correct_tracker_points\:room_estimation}
{\xe \v room_estimation\:correct_tracker_points}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
correct_tracker_points (  {\i self},   {\i df})}}
\par
{\bkmkstart AAAAAAAARW}
{\bkmkend AAAAAAAARW}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid ratios is the x,y|width,height ratios of the estimate to the ground truth.\par
Example: Estimates are recorded at 480x720 while ground truths are recorded at 720x1280. The ratio inputed would be (720/480, 1280/720) or (1.5, 1.777778)\par
\par
\par
invert_y is the height of the video. We record the data as if the origin is in the bottom left, but in other applications the origin is the top left.\par
This means we subtract the height of the video to inverse this effect.\par
If the height of the video is 720, invert_y=720. If the data being tested IS NOT inverted, leave it as 0 \par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2115     {\cf17 def }correct_tracker_points(self, df):\par
2116         {\cf22 """}\par
2117 {\cf22         ratios is the x,y|width,height ratios of the estimate to the ground truth.}\par
2118 {\cf22         Example: Estimates are recorded at 480x720 while ground truths are recorded at 720x1280. The ratio inputed would be (720/480, 1280/720) or (1.5, 1.777778)}\par
2119 {\cf22         }\par
2120 {\cf22 }\par
2121 {\cf22         invert_y is the height of the video. We record the data as if the origin is in the bottom left, but in other applications the origin is the top left.}\par
2122 {\cf22         This means we subtract the height of the video to inverse this effect.}\par
2123 {\cf22         If the height of the video is 720, invert_y=720. If the data being tested IS NOT inverted, leave it as 0 }\par
2124 {\cf22         """}\par
2125         width = int(df.iloc[0][{\cf22 'Width(px)'}])\par
2126         height = int(df.iloc[0][{\cf22 'Height(px)'}])\par
2127 \par
2128         df = df.iloc[1: , :] {\cf20 # Drop first row}\par
2129         \par
2130         df[{\cf22 'Pixel_Loc_x'}] = df[{\cf22 'Pixel_Loc_x'}].astype(int) * (width/df[{\cf22 'Max_Pixel_x'}].astype(int))\par
2131         df[{\cf22 'Pixel_Loc_y'}] = (df[{\cf22 'Max_Pixel_y'}].astype(int) - df[{\cf22 'Pixel_Loc_y'}].astype(int)) * (height/df[{\cf22 'Max_Pixel_y'}].astype(int))\par
2132         {\cf20 # point = (float(point['Pixel_Loc_x'])*ratios[0],}\par
2133         {\cf20 # (invert_y-float(point['y2']))*ratios[1],}\par
2134         {\cf20 #         float(point['x2'])*ratios[0],}\par
2135         {\cf20 # (invert_y-float(point['y1']))*ratios[1]}\par
2136         {\cf20 # )}\par
2137         print(df)\par
2138         {\cf19 return} df\par
2139 \par
}
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARV" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.load_tracker()}}}
.}\par
}
{\xe \v crop_plain\:room_estimation}
{\xe \v room_estimation\:crop_plain}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
crop_plain (  {\i self},   {\i points},   {\i plain_width},   {\i plain_length},   {\i image} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAAQJ}
{\bkmkend AAAAAAAAQJ}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Given 4 points the image will be cropped and reprojected as a rectangle.\par
\par
NOTE: Use length and height synonymously\par
\par
points = [(x1,y1), (x2,y2) ... (x4,y4)]\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 580     {\cf17 def }crop_plain(self, points, plain_width, plain_length, image=None):\par
581         {\cf22 '''}\par
582 {\cf22         Given 4 points the image will be cropped and reprojected as a rectangle.}\par
583 {\cf22 }\par
584 {\cf22         NOTE: Use length and height synonymously}\par
585 {\cf22 }\par
586 {\cf22         points = [(x1,y1), (x2,y2) ... (x4,y4)]}\par
587 {\cf22         '''}\par
588         {\cf19 if} image == {\cf18 None}:\par
589             image = self.image\par
590             \par
591         h, w = image.shape[:2]\par
592 \par
593         {\cf20 # These are the physical dimensions of the wall and the ratio in order to calculate the perspective}\par
594         plane_ratio = plain_width/plain_length\par
595 \par
596         {\cf20 # Perspective points are the points labelled multiplied by the ratio to get the end values of the points}\par
597         {\cf20 # We start with the bottom left of the wall, which means we use max_y of the wall room}\par
598 \par
599         {\cf22 '''}\par
600 {\cf22         2---3}\par
601 {\cf22         |   |}\par
602 {\cf22         1   4}\par
603 {\cf22         '''}\par
604 \par
605         p1 = (plain_length,0)\par
606         p2 = (0,0)\par
607         p3 = (plain_length, plain_width)\par
608         p4 = (0, plain_width)\par
609         perspective_points = np.array([p1,p2,p3,p4])\par
610 \par
611         {\cf20 # print(points)}\par
612         {\cf20 # print(perspective_points)}\par
613         \par
614 \par
615         points = np.array(points, dtype=np.float32)\par
616         perspective_points = np.array(perspective_points, dtype=np.float32)\par
617 \par
618         print(points.shape)\par
619         print(perspective_points.shape)\par
620         {\cf20 # use cv2.getPerspectiveTransform() to get M, the transform matrix, and Minv, the inverse}\par
621         M = cv2.getPerspectiveTransform(points, perspective_points)\par
622 \par
623         {\cf20 # use cv2.warpPerspective() to warp your image to a top-down view}\par
624         warped = cv2.warpPerspective(image, M, (w, h), flags=cv2.INTER_LINEAR)\par
625 \par
626         cv2.imshow({\cf22 "Wall Warped"}, warped)\par
627         cv2.waitKey(0)\par
628 \par
629         {\cf19 return} warped\par
630 \par
631 \par
}
{
\ql
References App.image, room_estimation.image, and person_tab.image.}\par
}
{\xe \v define_perspective_grid\:room_estimation}
{\xe \v room_estimation\:define_perspective_grid}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
define_perspective_grid (  {\i self},   {\i division} = {\f2 10},   {\i extend} = {\f2 500})}}
\par
{\bkmkstart AAAAAAAARK}
{\bkmkend AAAAAAAARK}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Assign 4 corners on an image where a grid will be projected into that space. This only selects the points and draws the edges. This should be called before superimpose_checker.\par
\par
See superimpose_checker for definition of the grid and how it works.\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1568     {\cf17 def }define_perspective_grid(self, division=10, extend = 500):\par
1569         {\cf22 '''}\par
1570 {\cf22         Assign 4 corners on an image where a grid will be projected into that space. This only selects the points and draws the edges. This should be called before superimpose_checker.}\par
1571 {\cf22 }\par
1572 {\cf22         See superimpose_checker for definition of the grid and how it works.}\par
1573 {\cf22         '''}\par
1574         wall = self.get_room_corners()\par
1575 \par
1576         {\cf20 # Describe a line with the two end points}\par
1577         fit1, equation1 = self.poly_fit_wall(wall, pixels_past_extent=1000, degree=1)\par
1578         line_1 = (fit1[0], fit1[-1])\par
1579         self.clear_corners()\par
1580 \par
1581         wall = self.get_room_corners()\par
1582         fit2, equation2 = self.poly_fit_wall(wall, pixels_past_extent=1000, degree=1)\par
1583         line_2 = (fit2[0], fit2[-1])\par
1584 \par
1585         intersection = self.find_intersection(equation1, equation2)\par
1586         self.show_img = cv2.circle(self.show_img, (int(intersection[0]), int(intersection[1])), 1,(0,255,0), 5, 0)\par
1587         print(intersection)\par
1588         \par
1589         {\cf20 # self.make_interpolater()}\par
1590         {\cf20 # x1i = interpolate.interp1d(line_1[0],line_1[1])}\par
1591         {\cf20 # y1i = interpolate.interp1d(line_2[0],line_2[1])}\par
1592 \par
1593         {\cf20 # x2i = interpolate.interp1d(line_1[0],line_2[0])}\par
1594         {\cf20 # y2i = interpolate.interp1d(line_1[1],line_2[1])}\par
1595 \par
1596         edge, step1 = np.linspace(line_1[0] - extend, line_2[0] + extend, division, endpoint={\cf17 True}, retstep={\cf17 True})\par
1597         edge2, step2 = np.linspace(line_1[1] - extend, line_2[1] + extend, division, endpoint={\cf17 True}, retstep={\cf17 True})\par
1598         \par
1599         {\cf19 for} i {\cf19 in} range(len(edge)):\par
1600             start = (int(edge[i][0]),int(edge[i][1]))\par
1601             end = (int(intersection[0]), int(intersection[1]))\par
1602             self.show_img = cv2.line(self.show_img, start, end, (255,0,0))\par
1603 \par
1604 \par
1605             start = (int(edge2[i][0]),int(edge2[i][1]))\par
1606             end = (int(intersection[0]), int(intersection[1]))\par
1607 \par
1608             self.show_img = cv2.line(self.show_img, start, end, (255,0,0))\par
1609 \par
1610             {\cf19 if} i % 10 == 0:\par
1611                 self.show_img = cv2.line(self.show_img, start, end, (0,100,0), thickness=2)\par
1612             cv2.imshow({\cf22 "test_interp"}, self.show_img)\par
1613             cv2.waitKey(1)\par
1614 \par
1615         self.clear_corners()\par
1616 \par
1617 \par
}
{
\ql
References room_estimation.clear_corners(), {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARH" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.find_intersection()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQD" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.get_room_corners()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARG" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.poly_fit_wall()}}}
, and room_estimation.show_img.}\par
}
{\xe \v define_room\:room_estimation}
{\xe \v room_estimation\:define_room}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
define_room (  {\i self},   {\i width} = {\f2 None},   {\i length} = {\f2 None},   {\i height} = {\f2 None},   {\i offset} = {\f2 [0,0,0,0]},   {\i corners} = {\f2 None},   {\i room_points} = {\f2 None},   {\i refine_corners} = {\f2 False},   {\i image} = {\f2 None},   {\i stitch_videos} = {\f2 None},   {\i calibration} = {\f2 None},   {\i extend} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAAQB}
{\bkmkend AAAAAAAAQB}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Takes variables and defines a room\par
\par
- width: and length are in mm\par
\par
- height: is the assumed height of all objects in the room\par
\par
- offset: is used when we extend the image to get all of the corners.\par
NOTE: If arrows do not line up with lines you drew, the camera matrix or the distortion matrix are off\par
\par
- room_points: are the real-world defined points in mm (if not defined points are the height and width of the room)\par
\par
- refine_corners: is opencv's sub-pixel optimizer to better estimate corner locations. This is mostly used in checkerboard calibration but may be useful when manually selecting corners.\par
\par
- calibration: is the calibration saved from a video (enter a video to calibrate, if calibration has been done save results in a pickle file)}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 248     {\cf17 def }define_room(self, width=None, length=None, height=None, offset=[0,0,0,0], corners=None, room_points=None, refine_corners=False, image=None, stitch_videos=None, calibration=None, extend=None):\par
249         {\cf22 '''}\par
250 {\cf22         Takes variables and defines a room}\par
251 {\cf22 }\par
252 {\cf22         - width: and length are in mm}\par
253 {\cf22 }\par
254 {\cf22         - height: is the assumed height of all objects in the room}\par
255 {\cf22 }\par
256 {\cf22         - offset: is used when we extend the image to get all of the corners.}\par
257 {\cf22         NOTE: If arrows do not line up with lines you drew, the camera matrix or the distortion matrix are off}\par
258 {\cf22 }\par
259 {\cf22         - room_points: are the real-world defined points in mm (if not defined points are the height and width of the room)}\par
260 {\cf22 }\par
261 {\cf22         - refine_corners: is opencv's sub-pixel optimizer to better estimate corner locations. This is mostly used in checkerboard calibration but may be useful when manually selecting corners.}\par
262 {\cf22 }\par
263 {\cf22         - calibration: is the calibration saved from a video (enter a video to calibrate, if calibration has been done save results in a pickle file)}\par
264 {\cf22 }\par
265 {\cf22         '''}\par
266 \par
267 \par
268         {\cf19 if} width {\cf19 is} {\cf19 not} {\cf18 None}:\par
269             self.room_width = width\par
270         {\cf19 if} length {\cf19 is} {\cf19 not} {\cf18 None}:\par
271             self.room_length = length\par
272         {\cf19 if} height {\cf19 is} {\cf19 not} {\cf18 None}:\par
273             self.room_height = height\par
274         \par
275         \par
276         {\cf19 if} calibration {\cf19 is} {\cf19 not} {\cf18 None}:\par
277             calibration_path = calibration[:-3] + {\cf22 "pickle"}\par
278             self.load_calibration(calibration_path)\par
279         \par
280 \par
281         {\cf20 # Stitch images takes care of undistortion based on calibration}\par
282         {\cf19 if} stitch_videos {\cf19 is} {\cf19 not} {\cf18 None}:\par
283 \par
284             self.image, self.stitch_homographies = self.stitch_rooms(stitch_videos, key_index=3)\par
285             self.show_img = deepcopy(self.image)\par
286 \par
287         \par
288         {\cf20 # Undistort images}\par
289         self.undistort_room()\par
290 \par
291         {\cf20 # find extention before selecting points so you do not have to calculate offset}\par
292         {\cf20 # extend is the number of walls you want to extend. Each wall intersection/extention requires 2 walls, 4 estimates each.}\par
293         offsets = [0,0,0,0]\par
294         {\cf19 if} extend {\cf19 is} {\cf19 not} {\cf18 None}:\par
295             {\cf19 for} i {\cf19 in} range(extend):\par
296                 intersection = self.find_wall_intersection(degree=1)\par
297                 {\cf20 # offset = [top, left, bottom, right] in pixels}\par
298                 self.image, offset = self.extend_image_to_corners(self.image, intersection)\par
299                 self.show_img =self.image\par
300                 {\cf20 # Offsets are the maximum offset values between all edges}\par
301                 {\cf19 for} index, value {\cf19 in} enumerate(offset):\par
302                     {\cf19 if} offsets[index] < value:\par
303                         offsets[index] = offset[index]\par
304 \par
305 \par
306         self.image = self.superimpose_checker()\par
307         self.show_img =self.image\par
308 \par
309         {\cf19 if} stitch_videos {\cf19 is} {\cf19 not} {\cf18 None}:\par
310             self.stitch_trackers(stitch_videos, self.stitch_homographies, offsets)\par
311 \par
312 \par
313         {\cf20 # creates a 4 corner room in real coordinates}\par
314         {\cf19 if} room_points {\cf19 is} {\cf19 not} {\cf18 None}:\par
315             print({\cf22 "Choose your points for the defined corners (in order)"})\par
316 \par
317             {\cf20 # Assign real world coordinates}\par
318             self.room_points = room_points\par
319 \par
320             {\cf20 # Assign a rectangular shape given room points (width and length)}\par
321             self.room_width, self.room_length = self.get_room_dimensions(room_points)\par
322             \par
323             {\cf20 # get 2D locations which will map to real world room points}\par
324             self.corners = self.get_room_corners(len(room_points))\par
325             room_points = np.reshape(np.asfarray(room_points), (len(room_points),3,1))\par
326             self.room_points = room_points\par
327 \par
328             \par
329         {\cf20 # }\par
330         {\cf19 elif} corners {\cf19 is} {\cf18 None}:\par
331             {\cf19 if} width == {\cf18 None} {\cf19 or} length == {\cf18 None}:\par
332                 room_points, self.corners = self.assign_non_square_corners()\par
333             \par
334             {\cf19 else}:\par
335                 room_points = self.get_room_points(self.room_width, self.room_length)\par
336                 self.corners = self.get_room_corners()\par
337         \par
338         print(self.corners)\par
339 \par
340         {\cf20 # '''}\par
341 \par
342         {\cf19 if} refine_corners:\par
343             {\cf20 # assert(image != None)}\par
344             {\cf19 if} image {\cf19 is} {\cf18 None}:\par
345                 image = self.image\par
346             {\cf20 # Refer to https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga354e0d7c86d0d9da75de9b9701a9a87e}\par
347             {\cf22 '''}\par
348 {\cf22             W FORSTNER. A fast operator for detection and precise location of distincs points, corners and center of circular features. In Proc. of the Intercommission Conference on Fast Processing of Photogrammetric Data, Interlaken, Switzerland, 1987, pages 281–305, 1987.}\par
349 {\cf22             '''}\par
350             {\cf20 # maxCorners = max(5000, 1)}\par
351             {\cf20 # # Parameters for Shi-Tomasi algorithm}\par
352             {\cf20 # qualityLevel = 0.01}\par
353             {\cf20 # minDistance = 10}\par
354             {\cf20 # blockSize = 3}\par
355             {\cf20 # gradientSize = 3}\par
356             {\cf20 # useHarrisDetector = False}\par
357             {\cf20 # k = 0.05}\par
358 \par
359             src_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\par
360 \par
361                 {\cf20 # Apply corner detection}\par
362             {\cf20 # corners = cv2.goodFeaturesToTrack(src_gray, maxCorners, qualityLevel, minDistance, None, \\}\par
363             {\cf20 #     blockSize=blockSize, gradientSize=gradientSize, useHarrisDetector=useHarrisDetector, k=k)}\par
364             {\cf20 # Draw corners detected}\par
365             {\cf20 # print('** Number of corners detected:', self.corners.shape[0])}\par
366             {\cf20 # radius = 4}\par
367             {\cf20 # for i in range(corners.shape[0]):}\par
368             {\cf20 #     cv2.circle(src_gray, (int(corners[i,0,0]), int(corners[i,0,1])), radius, (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)), cv2.FILLED)}\par
369             \par
370             {\cf20 # Set the needed parameters to find the refined corners}\par
371             winSize = (5, 5)\par
372             zeroZone = (-1, -1)\par
373             criteria = (cv2.TERM_CRITERIA_EPS + cv2.TermCriteria_COUNT, 40, 0.001)\par
374             {\cf20 # Calculate the refined corner locations}\par
375             src_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\par
376             self.corners = cv2.cornerSubPix(src_gray, self.corners, winSize, zeroZone, criteria)\par
377 \par
378             cv2.imshow({\cf22 "Corner Detection"}, src_gray)\par
379             cv2.waitKey(0)\par
380             \par
381 \par
382         {\cf20 # print(room_points)}\par
383         {\cf20 # self.corners, room_points = self.assign_non_square_corners()}\par
384         {\cf20 # # Correct for offsets with exteneded image}\par
385         {\cf20 # offset = [top, left, bottom, right]}\par
386         {\cf20 # for index, corner in enumerate(self.corners):}\par
387         {\cf20 #     x,y =self.corners[index]}\par
388         {\cf20 #     self.corners[index] = ((x - offset[1]), (y - offset[0]))}\par
389             \par
390         {\cf20 # print(room_points)}\par
391         {\cf20 # print(self.corners)}\par
392         key = {\cf18 None}\par
393         \par
394         {\cf19 while} {\cf17 True}:\par
395             test_image = deepcopy(self.image)\par
396             print({\cf22 "Room Dimensions"}, self.room_width, self.room_length)\par
397             room_points = self.get_room_points(self.room_width, self.room_length)\par
398             {\cf20 #NOTE This is calculated after distortion is corrected}\par
399             {\cf20 # Because we correct for this, distortion should be "None" as we undistort already. }\par
400             {\cf20 # retval, rvecs, tvecs = cv2.solvePnP(room_points.astype(np.float64), self.corners.astype(np.float64), self.camera_matrix, self.distioriton_matrix)}\par
401             {\cf20 # retval, rvecs, tvecs = cv2.solvePnP(room_points.astype(np.float64), self.corners.astype(np.float64), self.camera_matrix, None)}\par
402             retval, r1, t1 = cv2.solvePnP(room_points.astype(np.float64), self.corners.astype(np.float64), self.camera_matrix, {\cf18 None}, cv2.SOLVEPNP_IPPE, useExtrinsicGuess={\cf17 False})\par
403             retval, r2, t2 = cv2.solvePnP(room_points.astype(np.float64), self.corners.astype(np.float64), self.camera_matrix, {\cf18 None}, cv2.SOLVEPNP_EPNP, useExtrinsicGuess={\cf17 False})\par
404             retval, r3, t3 = cv2.solvePnP(room_points.astype(np.float64), self.corners.astype(np.float64), self.camera_matrix, {\cf18 None}, cv2.SOLVEPNP_ITERATIVE, useExtrinsicGuess={\cf17 False})\par
405             retval, r4, t4 = cv2.solvePnP(room_points.astype(np.float64), self.corners.astype(np.float64), self.camera_matrix, {\cf18 None}, cv2.SOLVEPNP_MAX_COUNT, useExtrinsicGuess={\cf17 False})\par
406             retval, r5, t5 = cv2.solvePnP(room_points.astype(np.float64), self.corners.astype(np.float64), self.camera_matrix, {\cf18 None}, cv2.SOLVEPNP_UPNP, useExtrinsicGuess={\cf17 False})\par
407             retval, r1, t1, inliers = cv2.solvePnPRansac(room_points.astype(np.float64), self.corners.astype(np.float64), self.camera_matrix, {\cf18 None})\par
408             img1, cube, target_cube = self.draw_cuboid(image=test_image, rvec=r1, tvec=t1)\par
409             cv2.imshow({\cf22 "Iterative Default"}, img1)\par
410             cv2.waitKey(0)\par
411             {\cf20 # Returns all solutions}\par
412             {\cf20 # retval, rall, tall, error = cv2.solvePnPGeneric(room_points.astype(np.float64), self.corners.astype(np.float64), self.camera_matrix, None)}\par
413             {\cf20 # for i in range(len(rall)):}\par
414                 \par
415             {\cf20 #     img1, cube, target_cube = self.draw_cuboid(image=test_image, rvec=rall[i], tvec=tall[i])}\par
416             {\cf20 #     cv2.imshow("Iterative Default", img1)}\par
417             {\cf20 #     cv2.waitKey(0)}\par
418             {\cf20 # # self.rotation_vector = rvecs}\par
419             {\cf20 # self.translation_vector = tvecs}\par
420 \par
421 \par
422 \par
423             {\cf19 if} key == ord({\cf22 'q'}):\par
424                 {\cf19 break}\par
425             {\cf19 if} key == ord({\cf22 '='}):\par
426                 self.room_width += 100\par
427             {\cf19 if} key == ord({\cf22 '-'}):\par
428                 self.room_width -= 100\par
429             \par
430             {\cf19 if} key == ord({\cf22 '0'}):\par
431                 self.room_length += 100\par
432             {\cf19 if} key == ord({\cf22 '9'}):\par
433                 self.room_length -= 100\par
434 \par
435             point = 0\par
436 \par
437             {\cf19 if} key == ord({\cf22 '1'}):\par
438                 point = 1\par
439             {\cf19 if} key == ord({\cf22 '2'}):\par
440                 point = 2\par
441             {\cf19 if} key == ord({\cf22 '3'}):\par
442                 point = 3\par
443             {\cf19 if} key == ord({\cf22 '4'}):\par
444                 point = 4\par
445             \par
446 \par
447             self.set_axis(self.room_width, self.room_height, self.room_length)\par
448             img1, cube, target_cube = self.draw_cuboid(image=test_image, rvec=r1, tvec=t1)\par
449             img2, cube, target_cube = self.draw_cuboid(image=test_image, rvec=r2, tvec=t2)\par
450             img3, cube, target_cube = self.draw_cuboid(image=test_image, rvec=r3, tvec=t3)\par
451             img4, cube, target_cube = self.draw_cuboid(image=test_image, rvec=r4, tvec=t4)\par
452             img5, cube, target_cube = self.draw_cuboid(image=test_image, rvec=r5, tvec=t5)\par
453 \par
454             cv2.imshow({\cf22 "IPPE"}, img1)\par
455             cv2.imshow({\cf22 "EPNP"}, img2)\par
456             cv2.imshow({\cf22 "ITERATIVE"}, img3)\par
457             cv2.imshow({\cf22 "MAX_COUNT"}, img4)\par
458             cv2.imshow({\cf22 "UPNP"}, img5)\par
459 \par
460             key = cv2.waitKey(1)\par
461 \par
}
}
{\xe \v define_sides\:room_estimation}
{\xe \v room_estimation\:define_sides}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
define_sides (  {\i self},   {\i sides},   {\i lengths})}}
\par
{\bkmkstart AAAAAAAAQA}
{\bkmkend AAAAAAAAQA}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 245     {\cf17 def }define_sides(self, sides, lengths):\par
246         {\cf19 pass}\par
247 \par
}
}
{\xe \v display_3D_Plot\:room_estimation}
{\xe \v room_estimation\:display_3D_Plot}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
display_3D_Plot (  {\i points},   {\i shown})}}
\par
{\bkmkstart AAAAAAAAQO}
{\bkmkend AAAAAAAAQO}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 755     {\cf17 def }display_3D_Plot(points, shown):\par
756         fig = plt.figure()\par
757         ax = plt.axes(projection={\cf22 '3d'})\par
758         x_data = points[0]\par
759         y_data = points[1]\par
760         z_data = points[2]\par
761         scatter = ax.scatter3D(x_data, y_data,z_data, cmap={\cf22 'Greens'})\par
762         \par
763         {\cf19 if} shown:\par
764             fig.canvas.draw()\par
765             fig.canvas.flush_events()\par
766         {\cf19 else}:\par
767             fig.show()\par
768 \par
769 \par
}
}
{\xe \v display_room\:room_estimation}
{\xe \v room_estimation\:display_room}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
display_room (  {\i self},   {\i image} = {\f2 None},   {\i axis} = {\f2 True},   {\i box} = {\f2 True},   {\i show_3d_plot} = {\f2 True},   {\i with_points} = {\f2 False})}}
\par
{\bkmkstart AAAAAAAAPW}
{\bkmkend AAAAAAAAPW}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 201     {\cf17 def }display_room(self, image=None, axis=True, box=True, show_3d_plot=True,  with_points=False):\par
202         {\cf19 while} {\cf17 True}:\par
203             \par
204             {\cf20 # self.draw_vector()}\par
205             {\cf19 if} axis:\par
206                 self.draw_axis()\par
207             \par
208             {\cf19 if} box:\par
209                 {\cf19 try}:\par
210                     self.draw_box()\par
211                 {\cf19 except}:\par
212                     {\cf19 pass}\par
213 \par
214             {\cf20 # if show_3d_plot:}\par
215             {\cf20 #     self.show_3D_plot()}\par
216             {\cf19 if} image {\cf19 is} {\cf19 not} {\cf18 None}:\par
217                 print({\cf22 "Show img"})\par
218                 cv2.imshow(self.window_name, image)\par
219             {\cf19 else}:\par
220                 cv2.imshow(self.window_name, self.show_img)\par
221             key = cv2.waitKey(1)\par
222 \par
223             {\cf19 if} key == ord({\cf22 'c'}):\par
224                 {\cf19 break}\par
225 \par
}
}
{\xe \v draw_axis\:room_estimation}
{\xe \v room_estimation\:draw_axis}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
draw_axis (  {\i self})}}
\par
{\bkmkstart AAAAAAAAQP}
{\bkmkend AAAAAAAAQP}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 770     {\cf17 def }draw_axis(self):\par
771         \par
772         projected, jac = cv2.projectPoints(self.axis, self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
773         corner = tuple(self.corners[0].ravel().astype({\cf22 'int32'}))\par
774         corner2 = tuple(projected[0].ravel().astype({\cf22 'int32'}))\par
775         corner3 = tuple(projected[1].ravel().astype({\cf22 'int32'}))\par
776         corner4 = tuple(projected[2].ravel().astype({\cf22 'int32'}))\par
777         \par
778         cv2.putText(self.show_img,{\cf22 "x"},corner2,fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=2, color=(255,0,0))\par
779         cv2.putText(self.show_img,{\cf22 "y"},corner4,fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=2, color=(0,255,0))\par
780         cv2.putText(self.show_img,{\cf22 "z"},corner3,fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=2, color=(0,0,255))\par
781 \par
782         self.show_img = cv2.arrowedLine(self.show_img, corner, corner2, (255,0,0), 2)\par
783         self.show_img = cv2.arrowedLine(self.show_img, corner, corner4, (0,255,0), 2)\par
784         self.show_img = cv2.arrowedLine(self.show_img, corner, corner3, (0,0,255), 2)\par
785 \par
786         {\cf19 return} self.show_img\par
787 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQM" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.get_3d_to_2d()}}}
.}\par
}
{\xe \v draw_box\:room_estimation}
{\xe \v room_estimation\:draw_box}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
draw_box (  {\i img},   {\i corners},   {\i imgpts})}}
\par
{\bkmkstart AAAAAAAAQR}
{\bkmkend AAAAAAAAQR}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 874     {\cf17 def }draw_box(img, corners, imgpts):\par
875         imgpts = np.int32(imgpts).reshape(-1,2)\par
876         {\cf20 # draw ground floor in green}\par
877         img = cv2.drawContours(img, [imgpts[:4]],-1,(0,255,0),-3)\par
878         {\cf20 # draw pillars in blue color}\par
879 \par
880         {\cf19 for} i,j {\cf19 in} zip(range(4),range(4,8)):\par
881             img = cv2.line(img, tuple(imgpts[i]), tuple(imgpts[j]),(255),3)\par
882         {\cf20 # draw top layer in red color}\par
883         img = cv2.drawContours(img, [imgpts[4:]],-1,(0,0,255),3)\par
884         {\cf19 return} img\par
885 \par
}
}
{\xe \v draw_cuboid\:room_estimation}
{\xe \v room_estimation\:draw_cuboid}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
draw_cuboid (  {\i self},   {\i image} = {\f2 None},   {\i distortion} = {\f2 None},   {\i rvec} = {\f2 None},   {\i tvec} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAAQQ}
{\bkmkend AAAAAAAAQQ}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid     Distortion is removed if image is undistorted.\par
    Rvec and Tvec are transformation vectors calculated by previous SolvePNP, so we use that if we have alerady done so.\par
    6        7\par
   *--------*\par
 / |       /|\par
2 *---------*3|\par
|  * 5    | *8\par
| /       |/\par
1 *---------*4}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 788     {\cf17 def }draw_cuboid(self, image=None, distortion=None, rvec=None, tvec=None):\par
789 \par
790         {\cf22 '''  }\par
791 {\cf22             Distortion is removed if image is undistorted.}\par
792 {\cf22             Rvec and Tvec are transformation vectors calculated by previous SolvePNP, so we use that if we have alerady done so.}\par
793 {\cf22             6        7}\par
794 {\cf22            *--------*}\par
795 {\cf22          / |       /|}\par
796 {\cf22       2 *---------*3|}\par
797 {\cf22         |  * 5    | *8}\par
798 {\cf22         | /       |/}\par
799 {\cf22       1 *---------*4}\par
800 {\cf22 }\par
801 {\cf22         '''}\par
802 \par
803         cube = np.float32([ [0,0,0], {\cf20 #1}\par
804                             [0,0,self.room_height], {\cf20 #2 Y}\par
805                             [self.room_width,0,self.room_height], {\cf20 # 3}\par
806                             [self.room_width,0,0], {\cf20 #4 X}\par
807 \par
808                             [0,self.room_length,0],\par
809                             [0,self.room_length,self.room_height],\par
810                             [self.room_width,self.room_length,self.room_height],\par
811                             [self.room_width,self.room_length,0]\par
812                             \par
813                             ]).reshape(-1,3)\par
814 \par
815         {\cf20 # retval, rvecs, tvecs, inliers = cv2.solvePnPRansac(cube.astype(np.float64), selected_2d.astype(np.float64), self.camera_matrix, distortion, rvec=rvec, tvec=tvec)}\par
816         {\cf20 # rvecs, tvecs = [None, None]}\par
817 \par
818 \par
819         {\cf20 # try:}\par
820         {\cf20 #     if distortion:}\par
821         {\cf20 #         retval, rvecs, tvecs = cv2.solvePnP(points.astype(np.float64), corners.astype(np.float64), self.camera_matrix, self.distioriton_matrix)}\par
822         {\cf20 #     else:}\par
823         {\cf20 #         retval, rvecs, tvecs =cv2.solvePnP(points.astype(np.float64), corners.astype(np.float64), self.camera_matrix, self.distioriton_matrix)}\par
824         {\cf20 # except:}\par
825         {\cf20 #     try:}\par
826         {\cf20 #         corners = np.reshape(np.asfarray(self.corners), (len(self.corners),2,1))}\par
827         {\cf20 #         retval, rvecs, tvecs = cv2.solvePnP(self.room_points.astype(np.float64), corners.astype(np.float64), self.camera_matrix, self.distioriton_matrix)}\par
828         {\cf20 #     except:}\par
829         {\cf20 #         corners = np.reshape(np.asfarray(self.corners), (len(self.corners),2,1))}\par
830 \par
831         {\cf20 #         points = self.get_n_room_points(self.room_points[:corners.shape[0]])}\par
832         {\cf20 #         retval, rvecs, tvecs = cv2.solvePnP(points.astype(np.float64), corners.astype(np.float64), self.camera_matrix, self.distioriton_matrix)}\par
833 \par
834                 \par
835         projected, jac = cv2.projectPoints(cube, rvec, tvec, self.camera_matrix, distortion)\par
836 \par
837 \par
838         \par
839         corner1 = tuple(projected[0].ravel().astype({\cf22 'int32'}))\par
840         corner2 = tuple(projected[1].ravel().astype({\cf22 'int32'}))\par
841         corner3 = tuple(projected[2].ravel().astype({\cf22 'int32'}))\par
842         corner4 = tuple(projected[3].ravel().astype({\cf22 'int32'}))\par
843         corner5 = tuple(projected[4].ravel().astype({\cf22 'int32'}))\par
844         corner6 = tuple(projected[5].ravel().astype({\cf22 'int32'}))\par
845         corner7 = tuple(projected[6].ravel().astype({\cf22 'int32'}))\par
846         corner8 = tuple(projected[7].ravel().astype({\cf22 'int32'}))\par
847 \par
848         projected_cube = [corner1, corner2, corner3, corner4, corner5, corner6, corner7, corner8]\par
849         \par
850         {\cf19 if} image {\cf19 is} {\cf18 None}:\par
851             image = self.show_img\par
852 \par
853         cv2.putText(image,{\cf22 "x"},corner4,fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=2, color=(255,25,200))\par
854         cv2.putText(image,{\cf22 "y"},corner2,fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=2, color=(0,255,0))\par
855         cv2.putText(image,{\cf22 "z"},corner5,fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=2, color=(0,0,255))\par
856 \par
857         image = cv2.line(image, corner1, corner2, (0,255,0), 2) {\cf20 # Y}\par
858         image = cv2.line(image, corner2, corner3, (255,0,0), 2)\par
859         image = cv2.line(image, corner3, corner4, (255,0,0), 2)\par
860         image = cv2.line(image, corner1, corner4, (255,25,200), 2) {\cf20 #X}\par
861 \par
862         image = cv2.line(image, corner5, corner6, (255,0,0), 2)\par
863         image = cv2.line(image, corner6, corner7, (255,0,0), 2)\par
864         image = cv2.line(image, corner7, corner8, (255,0,0), 2)\par
865         image = cv2.line(image, corner8, corner5, (255,0,0), 2)\par
866 \par
867         image = cv2.line(image, corner1, corner5, (0,0,255), 2) {\cf20 #Z}\par
868         image = cv2.line(image, corner2, corner6, (255,0,0), 2)\par
869         image = cv2.line(image, corner3, corner7, (255,0,0), 2)\par
870         image = cv2.line(image, corner4, corner8, (255,0,0), 2)\par
871 \par
872         {\cf19 return} image, cube, projected_cube\par
873 \par
}
{
\ql
References room_estimation.ax, room_estimation.back_plt, room_estimation.camera_matrix, room_estimation.distioriton_matrix, room_estimation.fig, room_estimation.float_plt, room_estimation.front_plt, room_estimation.line_plt, room_estimation.mapped, room_estimation.mapped_dictionary, room_estimation.points_plt, room_estimation.quiver, room_estimation.room_height, room_estimation.room_length, room_estimation.room_width, room_estimation.rotation_vector, room_estimation.show_3D_plot, room_estimation.show_img, room_estimation.translation_vector, room_estimation.vector_depth, and room_estimation.window_name.}\par
}
{\xe \v draw_grid\:room_estimation}
{\xe \v room_estimation\:draw_grid}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
draw_grid (  {\i self},   {\i grid_shape} = {\f2 (20,20)},   {\i color} = {\f2 (255,\~ 255,\~ 255)},   {\i thickness} = {\f2 2})}}
\par
{\bkmkstart AAAAAAAASG}
{\bkmkend AAAAAAAASG}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2704     {\cf17 def }draw_grid(self, grid_shape=(20,20), color=(255, 255, 255), thickness=2):\par
2705         h = 1080\par
2706         w = 1080\par
2707         {\cf20 # h, w, _ = img.shape}\par
2708         img = np.zeros((h, w, 3), dtype = {\cf22 "uint8"})\par
2709         rows, cols = grid_shape\par
2710         dy, dx = h / rows, w / cols\par
2711 \par
2712 \par
2713         {\cf20 # draw vertical lines}\par
2714         {\cf19 for} index, x {\cf19 in} enumerate(np.linspace(start=dx, stop=w-dx, num=cols-1)):\par
2715             {\cf19 if} index == (cols-1)/2:\par
2716                 x = int(round(x))\par
2717                 img = cv2.line(img, (x, 0), (x, h), color=(255,0,255), thickness=7)\par
2718             {\cf19 else}:\par
2719                 x = int(round(x))\par
2720                 img = cv2.line(img, (x, 0), (x, h), color=color, thickness=thickness)\par
2721 \par
2722         {\cf20 # draw horizontal lines}\par
2723         {\cf19 for} index, y {\cf19 in} enumerate(np.linspace(start=dy, stop=h-dy, num=rows-1)):\par
2724             {\cf19 if} index == (rows-1)/2:\par
2725                 y = int(round(y))\par
2726                 img = cv2.line(img, (0, y), (w, y), color=(255,0,255), thickness=7)\par
2727             {\cf19 else}:\par
2728                 y = int(round(y))\par
2729                 img = cv2.line(img, (0, y), (w, y), color=color, thickness=thickness)\par
2730 \par
2731         {\cf19 return} img\par
2732 \par
2733 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARJ" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.superimpose_checker()}}}
.}\par
}
{\xe \v draw_point\:room_estimation}
{\xe \v room_estimation\:draw_point}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
draw_point (  {\i self},   {\i img},   {\i front_pt},   {\i back_pt})}}
\par
{\bkmkstart AAAAAAAAQS}
{\bkmkend AAAAAAAAQS}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 886     {\cf17 def }draw_point(self, img, front_pt, back_pt):\par
887         img = cv2.circle(img, front_pt, 8,(125,0,125),2,0)\par
888         img = cv2.drawMarker(img, back_pt, color=(255,255,0),markerType=1)\par
889         img = cv2.line(img,front_pt, back_pt, (100,100,0), 2)\par
890         img = cv2.circle(img, front_pt, 8,(255,255,0),2,0)\par
891         {\cf19 return} img\par
892 \par
}
}
{\xe \v draw_vector\:room_estimation}
{\xe \v room_estimation\:draw_vector}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
draw_vector (  {\i self},   {\i event},   {\i x},   {\i y},   {\i flags},   {\i params})}}
\par
{\bkmkstart AAAAAAAAQU}
{\bkmkend AAAAAAAAQU}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 912     {\cf17 def }draw_vector(self,event,x,y,flags,params):\par
913         {\cf20 # self.show_img = self.img_copy()}\par
914         values = {\cf18 None}\par
915         x_list = []\par
916         y_list = []\par
917         z_list = []\par
918 \par
919         {\cf19 if} event == 10:\par
920             {\cf20 #sign of the flag shows direction of mousewheel}\par
921             {\cf19 if} flags > 0:\par
922                 self.vector_depth += 10\par
923             {\cf19 else}:\par
924                 self.vector_depth -= 10\par
925 \par
926 \par
927         front_point = np.float32([[x,0,y]])\par
928         \par
929         front, jac = cv2.projectPoints(front_point, self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
930 \par
931         back_point = np.float32([[x,self.room_length,y]])\par
932         back, jac = cv2.projectPoints(back_point,self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
933 \par
934         floater_point = np.float32([[x, self.vector_depth, y]])\par
935         floater, jac = cv2.projectPoints(floater_point, self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
936 \par
937         {\cf20 # print(x,y)}\par
938 \par
939         front_point = (front[0][0][0],front[0][0][1])\par
940         back_point = (back[0][0][0],back[0][0][1])\par
941         floater_point = (floater[0][0][0],floater[0][0][1])\par
942 \par
943 \par
944         self.show_img = cv2.circle(self.show_img, front_point, 8,(125,0,125),2,0)\par
945         self.show_img = cv2.drawMarker(self.show_img, back_point, color=(255,255,0),markerType=1)\par
946         self.show_img = cv2.line(self.show_img,front_point, back_point, (100,100,0), 2)\par
947         self.show_img = cv2.circle(self.show_img, front_point, 8,(255,255,0),2,0)\par
948         self.show_img = cv2.circle(self.show_img, floater_point, 8,(255,0,0),2,0)\par
949 \par
950         ifp = (int(front_point[0]), int(front_point[1]))\par
951         {\cf19 if} (x,y) {\cf19 in} self.mapped_dictionary.keys():\par
952             values = self.mapped_dictionary[(x,y)]\par
953             {\cf19 for} v {\cf19 in} values:\par
954                 self.show_img = cv2.drawMarker(self.show_img, (x,y), color=(0,0,255),markerType=3)\par
955                 x_list = []\par
956                 y_list = []\par
957                 z_list = []\par
958                 {\cf19 for} v {\cf19 in} values:\par
959                     x_list.append(v[0])\par
960                     y_list.append(v[2])\par
961                     z_list.append(v[1])\par
962 \par
963         cv2.imshow(self.window_name,self.show_img)\par
964         cv2.waitKey(1)\par
965         {\cf19 if} self.show_3D_plot == {\cf17 True}:\par
966             plt.ion\par
967 \par
968 \par
969             {\cf19 if} self.fig {\cf19 is} {\cf18 None}:\par
970                 self.fig = plt.figure(1)\par
971                 self.ax = ax = plt.axes(projection={\cf22 '3d'})\par
972                 self.front_plt = ax.scatter3D(front_point[0], front_point[1], 0)\par
973                 self.back_plt = ax.scatter3D(back_point[0],back_point[1], self.vector_depth)\par
974                 self.float_plt = ax.scatter3D(floater_point[0],floater_point[1],self.vector_depth)\par
975                 self.line_plt = ax.plot3D([front_point[0],back_point[0]], [front_point[1],back_point[1]], [0,self.room_length], color={\cf22 'teal'})\par
976                 \par
977                 {\cf20 # plt.quiver(*origin, V[:,0], V[:,1], color=['r','b','g'], scale=21)}\par
978                 self.ax.set_xlim(-self.room_width,self.room_width)\par
979                 self.ax.set_ylim(-self.room_height,self.room_width)\par
980                 self.ax.set_zlim(-self.room_length,self.room_width)\par
981                 self.fig.canvas.draw()\par
982                 self.fig.canvas.flush_events()\par
983                 self.fig.show()\par
984             {\cf19 else}:\par
985 \par
986                 \par
987                 \par
988                 self.ax.cla()\par
989                 {\cf19 if} values:\par
990                     self.points_plt = self.ax.scatter3D(x_list,y_list,z_list, marker={\cf22 "D"}, color={\cf22 "red"})\par
991                 {\cf20 # plt.quiver((0,0,0), (0,1,0), , color=['r','b','g'], scale=21)}\par
992                 self.ax.set_xlim(-self.room_width,self.room_width)\par
993                 self.ax.set_ylim(-self.room_height,self.room_width)\par
994                 self.ax.set_zlim(-self.room_length,self.room_width)\par
995                 self.ax.set_xlabel({\cf22 "X"})\par
996                 self.ax.set_ylabel({\cf22 "Y"})\par
997                 self.ax.set_zlabel({\cf22 "Z"})\par
998                 self.float_plt = self.ax.scatter3D(floater_point[0],floater_point[1],self.vector_depth, marker={\cf22 "o"}, color={\cf22 "blue"})\par
999                 self.front_plt = self.ax.scatter3D(front_point[0], front_point[1], 0, marker=({\cf22 "o"}), color={\cf22 "cyan"})\par
1000                 self.back_plt = self.ax.scatter3D(back_point[0],back_point[1], self.room_length, marker={\cf22 "x"}, color={\cf22 "cyan"})\par
1001                 self.line_plt = self.ax.plot3D([front_point[0],back_point[0]], [front_point[1],back_point[1]], [0,self.room_length], color={\cf22 'teal'})\par
1002                 self.quiver = self.ax.quiver([0,0,0], [0,0,0], [0,0,0], [self.room_width,0,0], [0,self.room_height,0], [0,0,self.room_length], length=0.1, normalize={\cf17 False})\par
1003                 self.fig.canvas.draw()\par
1004                 self.fig.canvas.flush_events()\par
1005 \par
1006         {\cf19 return} self.show_img\par
1007 \par
1008 \par
}
}
{\xe \v edit_room_corner\:room_estimation}
{\xe \v room_estimation\:edit_room_corner}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
edit_room_corner (  {\i self},   {\i index},   {\i new_pixel})}}
\par
{\bkmkstart AAAAAAAAQE}
{\bkmkend AAAAAAAAQE}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 501     {\cf17 def }edit_room_corner(self, index, new_pixel):\par
502         self.corners[index][0] = new_pixel[0]\par
503         self.corners[index][1] = new_pixel[1]\par
504         {\cf19 return} self.corners\par
505 \par
}
}
{\xe \v estimate\:room_estimation}
{\xe \v room_estimation\:estimate}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
estimate (  {\i self})}}
\par
{\bkmkstart AAAAAAAARE}
{\bkmkend AAAAAAAARE}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Depreciated...\par
\par
NOTE: define room first\par
\par
This defines room limits and projects 3D points onto the screen\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1432     {\cf17 def }estimate(self):\par
1433         {\cf22 '''}\par
1434 {\cf22         Depreciated...}\par
1435 {\cf22 }\par
1436 {\cf22         NOTE: define room first}\par
1437 {\cf22         }\par
1438 {\cf22         This defines room limits and projects 3D points onto the screen}\par
1439 {\cf22         '''}\par
1440         print({\cf22 "Finding Limits..."})\par
1441         limits, _ = self.find_3d_limits()\par
1442         print({\cf22 "Limits found..."})\par
1443 \par
1444         print({\cf22 "Estimating 3D space within limits..."})\par
1445         self.get_room_3d(limits, height=self.room_height, step=2)\par
1446         print({\cf22 "Estimation complete.."})\par
1447 \par
1448         cv2.imshow({\cf22 "Points"}, self.show_img)\par
1449         cv2.setMouseCallback({\cf22 'Points'}, self.get_depth)\par
1450         cv2.waitKey(0)\par
1451     \par
}
{
\ql
References room_estimation.find_3d_limits(), {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARD" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.get_depth()}}}
, room_estimation.get_depth, room_estimation.get_room_3d(), room_estimation.room_height, and room_estimation.show_img.}\par
}
{\xe \v estimate_plane\:room_estimation}
{\xe \v room_estimation\:estimate_plane}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
estimate_plane (  {\i img},   {\i corners},   {\i points},   {\i calibration_matrix},   {\i distortion_matrix})}}
\par
{\bkmkstart AAAAAAAAQT}
{\bkmkend AAAAAAAAQT}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 893     {\cf17 def }estimate_plane(img, corners, points, calibration_matrix, distortion_matrix):\par
894 \par
895         criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\par
896         objp = np.zeros((6*7,3), np.float32)\par
897         objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\par
898         \par
899         axis = np.array([[3,0,0], [0,3,0], [0,0,-3]],dtype=np.float32).reshape(-1,3)\par
900         \par
901         corners2 = cv2.cornerSubPix(img,corners,(11,11),(-1,-1),criteria)\par
902         {\cf20 # Find the rotation and translation vectors.}\par
903         ret, rvecs, tvecs = cv2.solvePnPRansac(objp, corners2, calibration_matrix, distortion_matrix)\par
904         \par
905 \par
906         print({\cf22 "CalibRVEC:"}, rvecs, {\cf22 "CALIBTVEC"}, tvecs)\par
907         {\cf20 # project 3D points to image plane}\par
908         imgpts, jac = cv2.projectPoints(axis, rvecs, tvecs, calibration_matrix, distortion_matrix)\par
909 \par
910         {\cf19 return} imgpts, jac\par
911 \par
}
}
{\xe \v evaluate_calibration\:room_estimation}
{\xe \v room_estimation\:evaluate_calibration}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
evaluate_calibration (  {\i self})}}
\par
{\bkmkstart AAAAAAAASC}
{\bkmkend AAAAAAAASC}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2324     {\cf17 def }evaluate_calibration(self):\par
2325         {\cf19 pass}\par
2326 \par
}
}
{\xe \v extend_image_to_corners\:room_estimation}
{\xe \v room_estimation\:extend_image_to_corners}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
extend_image_to_corners (  {\i self},   {\i image},   {\i corners})}}
\par
{\bkmkstart AAAAAAAASA}
{\bkmkend AAAAAAAASA}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid extends the image with black until corners are visible.\par
\par
Returns new image as well as the amount extended and on which sides. This is useful for transforming datapoints.\par
\par
offset = [top, left, bottom, right] in pixels\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2263     {\cf17 def }extend_image_to_corners(self, image, corners):\par
2264         {\cf22 '''}\par
2265 {\cf22         extends the image with black until corners are visible.}\par
2266 {\cf22 }\par
2267 {\cf22         Returns new image as well as the amount extended and on which sides. This is useful for transforming datapoints.}\par
2268 {\cf22 }\par
2269 {\cf22         offset = [top, left, bottom, right] in pixels}\par
2270 {\cf22         '''}\par
2271         min_x = 0\par
2272         min_y = 0\par
2273         max_x = 0\par
2274         max_y = 0\par
2275 \par
2276         {\cf19 for} corner {\cf19 in} corners:\par
2277             {\cf19 if} corner[0] < min_x:\par
2278                 min_x = corner[0]\par
2279             {\cf19 if} corner[1] < min_y:\par
2280                 min_y = corner[1]\par
2281             {\cf19 if} corner[0] > max_x:\par
2282                 max_x = corner[0]\par
2283             {\cf19 if} corner[1] > max_y:\par
2284                 max_y = corner[1]\par
2285 \par
2286         {\cf19 if} max_y < image.shape[0]:\par
2287             max_y = image.shape[0]\par
2288         {\cf19 if} max_x < image.shape[1]:\par
2289             max_x = image.shape[1]\par
2290 \par
2291         offset = (abs(int(min_y)), abs(int(min_x)), abs(int(max_y - image.shape[0])), abs(int(max_x - image.shape[1])))\par
2292         image = cv2.copyMakeBorder(image, top=offset[0], left=offset[1], bottom=offset[2], right=offset[3], borderType=0)\par
2293         \par
2294 \par
2295         {\cf19 for} corner_1 {\cf19 in} corners:\par
2296             p1 = (int(corner_1[0] + offset[1]), int(corner_1[1] + offset[0]))\par
2297             image = image = cv2.circle(image, p1, 2, (255,255,0), 2)\par
2298             {\cf19 for} corner_2 {\cf19 in} corners:\par
2299                 p1 = (int(corner_1[0] + offset[1]), int(corner_1[1] + offset[0]))\par
2300                 p2 = (int(corner_2[0] + offset[1]), int(corner_2[1] + offset[0]))\par
2301                 image = cv2.line(image, p1, p2, (255,0,0))\par
2302         \par
2303         {\cf20 #Resets corners for future assignment}\par
2304         self.clear_corners()\par
2305         {\cf19 return} image, offset\par
2306 \par
}
{
\ql
References room_estimation.clear_corners().}\par
}
{\xe \v find_3d_limits\:room_estimation}
{\xe \v room_estimation\:find_3d_limits}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
find_3d_limits (  {\i self},   {\i height} = {\f2 1000})}}
\par
{\bkmkstart AAAAAAAARB}
{\bkmkend AAAAAAAARB}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1204     {\cf17 def }find_3d_limits(self, height=1000):\par
1205         print({\cf22 "Finding Limits..."})\par
1206 \par
1207         edge_buffer = 5\par
1208 \par
1209         x_limit = self.show_img.shape[1] - edge_buffer\par
1210         y_limit = self.show_img.shape[0] - edge_buffer\par
1211         x_min = 5\par
1212         y_min = 5\par
1213         \par
1214         bottom_left = (0,0,height)\par
1215         top_left = (0,self.room_length,height)\par
1216         top_right = (self.room_width,self.room_length,height)\par
1217         bottom_right = (self.room_width,0,height)\par
1218 \par
1219         test_step = 100 {\cf20 # 10cm}\par
1220         max_distance_test = 12000 {\cf20 # 12m max}\par
1221         {\cf22 '''}\par
1222 {\cf22         Get top left}\par
1223 {\cf22         '''}\par
1224         {\cf20 #test left limits}\par
1225         pixel_y = inf\par
1226         pixel_x = inf\par
1227 \par
1228         {\cf20 # test_top left}\par
1229         x_lim = top_left[0]\par
1230         y_lim = top_left[1]\par
1231         z_lim = 0\par
1232 \par
1233 \par
1234         \par
1235         {\cf19 while} abs(x_lim) <= float(max_distance_test) {\cf19 and} abs(y_lim) <= float(max_distance_test) {\cf19 and} abs(z_lim):\par
1236             print(x_lim, y_lim, z_lim, max_distance_test)\par
1237             {\cf20 # grab x and y of }\par
1238             x_lim = top_left[0]\par
1239             y_lim = top_left[1]\par
1240             z_lim = top_left[2]\par
1241 \par
1242 \par
1243             point_lim = np.float32([x_lim, y_lim , height])\par
1244 \par
1245             mapped_point, jac = cv2.projectPoints(point_lim, self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
1246             pixel_y = mapped_point[0][0][1]\par
1247             pixel_x = mapped_point[0][0][0]\par
1248             {\cf19 if} pixel_x >= edge_buffer:\par
1249                 top_left = (top_left[0] - test_step, top_left[1], top_left[2])\par
1250             {\cf19 else}:\par
1251                 top_left = (top_left[0] + test_step/2, top_left[1], top_left[2])\par
1252             {\cf20 # if pixel_x < 10:}\par
1253             {\cf20 #     print("X reached")}\par
1254                 \par
1255             \par
1256             {\cf19 if} pixel_y >= edge_buffer:\par
1257                 top_left = (top_left[0], top_left[1] + test_step, top_left[2])\par
1258             {\cf19 else}:\par
1259                 top_left = (top_left[0], top_left[1] - test_step/2, top_left[2])\par
1260             {\cf20 # if pixel_y < 10:}\par
1261             {\cf20 #     print("Y reached")}\par
1262             {\cf19 try}:\par
1263                 self.show_img = cv2.circle(self.show_img, (mapped_point[0][0][0], mapped_point[0][0][1]), 1,(255,0,0),1,0)\par
1264                 cv2.imshow({\cf22 'Points'}, self.show_img)\par
1265                 cv2.waitKey(1)\par
1266             {\cf19 except}:\par
1267                 {\cf19 pass}\par
1268             {\cf20 # print(pixel_x, pixel_y,top_left)}\par
1269 \par
1270             {\cf19 if} pixel_x < edge_buffer {\cf19 and} pixel_y < edge_buffer:\par
1271                 {\cf19 break}\par
1272         print({\cf22 "Top Left:"}, top_left)\par
1273 \par
1274         {\cf22 '''}\par
1275 {\cf22         Get top right}\par
1276 {\cf22         '''}\par
1277         pixel_y = inf\par
1278         pixel_x = -inf\par
1279         x_lim = top_right[0]\par
1280         y_lim = top_right[1]\par
1281         z_lim = top_right[2]\par
1282         {\cf20 # top_right = top_left}\par
1283         {\cf19 while} abs(x_lim) <= max_distance_test {\cf19 and} abs(y_lim) <= max_distance_test {\cf19 and} abs(z_lim) <= max_distance_test:\par
1284             print(x_lim, y_lim)\par
1285             {\cf20 # grab x and y of }\par
1286             x_lim = top_right[0]\par
1287             y_lim = top_right[1]\par
1288             z_lim = top_right[2]\par
1289 \par
1290             {\cf19 if} x_lim <= max_distance_test {\cf19 or} y_lim <= max_distance_test {\cf19 or} abs(z_lim) <= max_distance_test:\par
1291 \par
1292                 point_lim = np.float32([x_lim, y_lim , height])\par
1293 \par
1294                 mapped_point, jac = cv2.projectPoints(point_lim, self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
1295                 pixel_y = mapped_point[0][0][1]\par
1296                 pixel_x = mapped_point[0][0][0]\par
1297                 {\cf19 if} pixel_x <= x_limit:\par
1298                     top_right = (top_right[0] + test_step, top_right[1], top_right[2])\par
1299                 {\cf19 else}:\par
1300                     top_right = (top_right[0] - test_step/2, top_right[1], top_right[2])\par
1301 \par
1302                 {\cf19 if} pixel_y >= edge_buffer:\par
1303                     top_right = (top_right[0], top_right[1] + test_step, top_right[2])\par
1304                 {\cf19 else}:\par
1305                     top_right = (top_right[0], top_right[1] - test_step/2, top_right[2])\par
1306                 {\cf20 # if pixel_y > y_limit:}\par
1307                 {\cf20 #     print("Y reached")}\par
1308 \par
1309             {\cf19 try}:\par
1310                 self.show_img = cv2.circle(self.show_img, (mapped_point[0][0][0], mapped_point[0][0][1]), 1,(255,0,0),1,0)\par
1311                 cv2.imshow({\cf22 'Points'}, self.show_img)\par
1312                 cv2.waitKey(1)\par
1313             {\cf19 except}:\par
1314                 {\cf19 pass}\par
1315                 \par
1316                 {\cf19 if} pixel_x >= x_limit {\cf19 and} pixel_y < edge_buffer:\par
1317                     {\cf19 break}\par
1318         print({\cf22 "Top Right"}, top_right)\par
1319 \par
1320         {\cf22 '''}\par
1321 {\cf22         Get bottom left}\par
1322 {\cf22         '''}\par
1323         pixel_y = -inf\par
1324         pixel_x = -inf\par
1325         x_lim = bottom_left[0]\par
1326         y_lim = bottom_left[1]\par
1327         z_lim = bottom_left[2]\par
1328         {\cf20 # bottom_left = top_left}\par
1329         {\cf19 while} abs(x_lim) <= max_distance_test {\cf19 and} abs(y_lim) <= max_distance_test {\cf19 and} abs(z_lim) <= max_distance_test:\par
1330             {\cf20 # grab x and y of }\par
1331             x_lim = bottom_left[0]\par
1332             y_lim = bottom_left[1]\par
1333 \par
1334             point_lim = np.float32([x_lim, y_lim , height])\par
1335 \par
1336             mapped_point, jac = cv2.projectPoints(point_lim, self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
1337             pixel_y = mapped_point[0][0][1]\par
1338             pixel_x = mapped_point[0][0][0]\par
1339             {\cf19 if} pixel_x >= edge_buffer:\par
1340                 bottom_left = (bottom_left[0] - test_step, bottom_left[1], bottom_left[2])\par
1341             {\cf19 else}:\par
1342                 bottom_left = (bottom_left[0] + test_step/2, bottom_left[1], bottom_left[2])\par
1343 \par
1344 \par
1345             \par
1346             {\cf19 if} pixel_y <= y_limit:\par
1347                 bottom_left = (bottom_left[0], bottom_left[1] - test_step, bottom_left[2])\par
1348             {\cf19 else}:\par
1349                 bottom_left = (bottom_left[0], bottom_left[1] + test_step/2, bottom_left[2])\par
1350             {\cf20 # if pixel_y > y_limit:}\par
1351             {\cf20 #     print("Y reached")}\par
1352 \par
1353             {\cf19 try}:\par
1354                 self.show_img = cv2.circle(self.show_img, (mapped_point[0][0][0], mapped_point[0][0][1]), 1,(255,0,0),1,0)\par
1355                 cv2.imshow({\cf22 'Points'}, self.show_img)\par
1356                 cv2.waitKey(1)\par
1357             {\cf19 except}:\par
1358                 {\cf19 pass}\par
1359             {\cf20 # print(pixel_x, pixel_y,bottom_left)}\par
1360 \par
1361             {\cf19 if} pixel_x < edge_buffer {\cf19 and} pixel_y > y_limit:\par
1362                 {\cf19 break}\par
1363         print({\cf22 "Bottom Left:"}, bottom_left)\par
1364 \par
1365         {\cf22 '''}\par
1366 {\cf22         Get bottom right}\par
1367 {\cf22         '''}\par
1368         pixel_y = -inf\par
1369         pixel_x = -inf\par
1370         {\cf20 # bottom_right = bottom_left}\par
1371         x_lim = bottom_right[0]\par
1372         y_lim = bottom_right[1]\par
1373         {\cf19 while} abs(x_lim) <= max_distance_test {\cf19 and} abs(y_lim) <= max_distance_test {\cf19 and} abs(z_lim) <= max_distance_test:\par
1374             {\cf20 # grab x and y of }\par
1375             x_lim = bottom_right[0]\par
1376             y_lim = bottom_right[1]\par
1377 \par
1378             point_lim = np.float32([x_lim, y_lim , height])\par
1379 \par
1380             mapped_point, jac = cv2.projectPoints(point_lim, self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
1381             pixel_y = mapped_point[0][0][1]\par
1382             pixel_x = mapped_point[0][0][0]\par
1383             {\cf19 if} pixel_x <= x_limit:\par
1384                 bottom_right = (bottom_right[0] + test_step, bottom_right[1], bottom_right[2])\par
1385             {\cf19 else}:\par
1386                 bottom_right = (bottom_right[0] - test_step/2, bottom_right[1], bottom_right[2])\par
1387    \par
1388             {\cf19 if} pixel_y <= y_limit:\par
1389                 bottom_right = (bottom_right[0], bottom_right[1] - test_step, bottom_right[2])\par
1390             {\cf19 else}:\par
1391                 bottom_right = (bottom_right[0], bottom_right[1] + test_step/2, bottom_right[2])\par
1392             {\cf20 # if pixel_y > y_limit:}\par
1393             {\cf20 #     print("Y reached")}\par
1394 \par
1395             {\cf19 try}:\par
1396                 self.show_img = cv2.circle(self.show_img, (mapped_point[0][0][0], mapped_point[0][0][1]), 1,(255,0,0),1,0)\par
1397                 cv2.imshow({\cf22 'Points'}, self.show_img)\par
1398                 cv2.waitKey(1)\par
1399             {\cf19 except}:\par
1400                 {\cf19 pass}\par
1401             {\cf20 # print(pixel_x, pixel_y,bottom_right)}\par
1402 \par
1403             {\cf19 if} pixel_x >= x_limit {\cf19 and} pixel_y >= y_limit:\par
1404                 {\cf19 break}\par
1405         print({\cf22 "Bottom Right:"}, bottom_right)\par
1406         min = np.array((top_left, bottom_left, top_right, bottom_right))\par
1407         min = np.amin(min, 0)\par
1408 \par
1409         max = np.array((top_left, bottom_left, top_right, bottom_right))\par
1410         max = np.amax(max, 0)\par
1411 \par
1412         print(min, max)\par
1413         {\cf19 return} (min, max), (top_left, bottom_left, top_right, bottom_right)\par
1414 \par
1415 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARE" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.estimate()}}}
.}\par
}
{\xe \v find_intersection\:room_estimation}
{\xe \v room_estimation\:find_intersection}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
find_intersection (  {\i self},   {\i line_equation1},   {\i line_equation2})}}
\par
{\bkmkstart AAAAAAAARH}
{\bkmkend AAAAAAAARH}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Returns the (x,y) location of the intersection of 2 lines given m and b of both lines from slope intercept form (m*x + b)\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1487     {\cf17 def }find_intersection(self, line_equation1, line_equation2):\par
1488         {\cf22 """}\par
1489 {\cf22         Returns the (x,y) location of the intersection of 2 lines given m and b of both lines from slope intercept form (m*x + b)}\par
1490 {\cf22         """}\par
1491         m1 = line_equation1.coef[0]\par
1492         b1 = line_equation1.coef[1]\par
1493 \par
1494         m2 = line_equation2.coef[0]\par
1495         b2 = line_equation2.coef[1]\par
1496 \par
1497         {\cf20 # x intersection}\par
1498         xi = (b1-b2) / (m2-m1)\par
1499 \par
1500         {\cf20 #y intersection}\par
1501         yi = m1 * xi + b1\par
1502 \par
1503         print({\cf22 "INTERSECTION"}, xi, yi)\par
1504         {\cf19 return} (xi, yi)\par
1505 \par
}
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARK" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.define_perspective_grid()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARL" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.find_wall_intersection()}}}
.}\par
}
{\xe \v find_wall_intersection\:room_estimation}
{\xe \v room_estimation\:find_wall_intersection}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
find_wall_intersection (  {\i self},   {\i number_of_walls} = {\f2 2},   {\i samples_per_wall} = {\f2 4},   {\i extent} = {\f2 10000},   {\i intersection_pairs} = {\f2 []},   {\i degree} = {\f2 1})}}
\par
{\bkmkstart AAAAAAAARL}
{\bkmkend AAAAAAAARL}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Using samples along visible parts of walls, it is possible to fit the curve and extend the visible location of the wall\par
\par
\par
intersection_pairs is a list of pairs of lines which are to be checked for intersections\par
Because we only check the current, previous and beginning and last, we leave out all other instances so a specification of which pairs are desired is optional\par
[(0,3), (1,3)] \par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1618     {\cf17 def }find_wall_intersection(self, number_of_walls=2, samples_per_wall=4, extent=10000, intersection_pairs=[], degree=1):\par
1619         {\cf22 '''}\par
1620 {\cf22         Using samples along visible parts of walls, it is possible to fit the curve and extend the visible location of the wall}\par
1621 {\cf22 }\par
1622 {\cf22 }\par
1623 {\cf22         intersection_pairs is a list of pairs of lines which are to be checked for intersections}\par
1624 {\cf22         Because we only check the current, previous and beginning and last, we leave out all other instances so a specification of which pairs are desired is optional}\par
1625 {\cf22         [(0,3), (1,3)] }\par
1626 {\cf22         '''}\par
1627         fit_list = []\par
1628         corners = []\par
1629         {\cf20 # while True:}\par
1630         {\cf19 for} wall {\cf19 in} range(number_of_walls):\par
1631             print({\cf22 "Press C when done, 4 points per wall"})\par
1632             self.clear_corners()\par
1633             print({\cf22 "Finding Wall intersection"})\par
1634 \par
1635 \par
1636             wall = self.get_room_corners()\par
1637 \par
1638             fit, equation = self.poly_fit_wall(wall, pixels_past_extent=extent, degree=degree)\par
1639             fit_list.append(equation)\par
1640             {\cf19 if} len(fit_list) >= 2:\par
1641                 {\cf19 for} index, f1 {\cf19 in} enumerate(fit_list):\par
1642 \par
1643                     {\cf20 # test only when multiple lines exist}\par
1644                     {\cf19 if} index > 0:\par
1645 \par
1646                         {\cf20 #look for intersection between current and previous}\par
1647                         intersection = self.find_intersection(fit_list[index],fit_list[index-1])\par
1648                         {\cf19 if} {\cf19 not} math.isnan(intersection[0]):\par
1649                             corners.append(intersection)\par
1650                             cv2.circle(self.show_img, (int(intersection[0]), int(intersection[1])), 1,(0,255,0), 5, 0)\par
1651 \par
1652                     {\cf20 # test between the beginning and end}\par
1653                     {\cf19 if} index+1 == len(fit_list):\par
1654                         intersection = self.find_intersection(fit_list[index],fit_list[0])\par
1655                         {\cf19 if} {\cf19 not} math.isnan(intersection[0]):\par
1656                             corners.append(intersection)\par
1657                             cv2.circle(self.show_img, (int(intersection[0]), int(intersection[1])), 1,(0,255,0), 5, 0)\par
1658 \par
1659             end_1 = fit[0]\par
1660             end_2 = fit[-1]\par
1661             print(end_1, end_2)\par
1662             {\cf20 # print(fit.tolist())}\par
1663             \par
1664             self.connect_points(list(fit.astype(np.int).tolist()))\par
1665             key = cv2.waitKey(1) & 0xFF\par
1666             {\cf19 if} key == ord({\cf22 "d"}):\par
1667                 {\cf19 break}\par
1668         \par
1669         {\cf20 # Add all of intersection_pairs that are specified}\par
1670         {\cf19 for} intersection {\cf19 in} intersection_pairs:\par
1671             intersection = self.find_intersection(fit_list[index],fit_list[0])\par
1672             corners.append(intersection)\par
1673             cv2.circle(self.show_img, (int(intersection[0]), int(intersection[1])), 1,(0,255,0), 5, 0)\par
1674 \par
1675         {\cf20 # self.display_room()}\par
1676         {\cf19 return} corners\par
1677         {\cf20 # self.connect_points(fit)}\par
1678 \par
}
{
\ql
References room_estimation.calibration_resolution, room_estimation.camera_matrix, room_estimation.clear_corners(), room_estimation.connect_points(), room_estimation.distioriton_matrix, room_estimation.error_list, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARH" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.find_intersection()}}}
, room_estimation.get_checkerboard_shape(), {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQD" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.get_room_corners()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARG" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.poly_fit_wall()}}}
, room_estimation.rotation_vector, room_estimation.save_calibration(), room_estimation.show_img, and room_estimation.translation_vector.}\par
}
{\xe \v fisheye_calibrate\:room_estimation}
{\xe \v room_estimation\:fisheye_calibrate}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
fisheye_calibrate (  {\i self},   {\i checkerboard_images},   {\i checkerboard_grid} = {\f2 None},   {\i out} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAARN}
{\bkmkend AAAAAAAARN}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1699     {\cf17 def }fisheye_calibrate(self, checkerboard_images, checkerboard_grid=None, out=None):\par
1700         {\cf20 # Checkboard dimensions}\par
1701         {\cf19 if} checkerboard_grid {\cf19 is} {\cf18 None}:\par
1702             CHECKERBOARD = self.get_checkerboard_shape(checkerboard_images)\par
1703         {\cf19 else}:\par
1704             CHECKERBOARD = checkerboard_grid\par
1705 \par
1706         \par
1707         subpix_criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.1)\par
1708         criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.1)\par
1709         calibration_flags = cv2.fisheye.CALIB_RECOMPUTE_EXTRINSIC + cv2.fisheye.CALIB_CHECK_COND + cv2.fisheye.CALIB_FIX_SKEW\par
1710         objp = np.zeros((1, CHECKERBOARD[0]*CHECKERBOARD[1], 3), np.float32)\par
1711         objp[0,:,:2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\par
1712 \par
1713         objpoints = [] {\cf20 # 3d point in real world space}\par
1714         imgpoints = [] {\cf20 # 2d points in image plane.}\par
1715 \par
1716         imgshape = {\cf18 None}\par
1717         {\cf19 for} image {\cf19 in} checkerboard_images:\par
1718 \par
1719             height, width = image.shape[:2]\par
1720 \par
1721             image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\par
1722 \par
1723             cv2.imshow({\cf22 'img'},image)\par
1724             cv2.waitKey(1)\par
1725             imgshape = image.shape\par
1726             retval, corners = cv2.findChessboardCorners(image, CHECKERBOARD)\par
1727             {\cf20 # cv2.cornerSubPix(checkerboard_image, corners)}\par
1728 \par
1729             {\cf20 # retval, cameraMatrix, distCoeffs, rvecs, tvecs = cv2.calibrateCamera(objectPoints, imagePoints, imageSize)}\par
1730             objp = np.zeros((1, CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32)\par
1731             objp[0,:,:2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\par
1732             prev_img_shape = {\cf18 None}\par
1733      \par
1734 \par
1735 \par
1736             {\cf19 if} retval == {\cf17 True}:\par
1737                 objpoints.append(objp)\par
1738                 {\cf20 # refining pixel coordinates for given 2d points.}\par
1739                 corners2 = cv2.cornerSubPix(image, corners, (11,11),(-1,-1), subpix_criteria)  \par
1740                 imgpoints.append(corners2)\par
1741                 {\cf20 # Draw and display the corners}\par
1742 \par
1743                 image = cv2.drawChessboardCorners(image, CHECKERBOARD, corners2, retval)\par
1744                 cv2.imshow({\cf22 "Calibration"}, image)\par
1745                 cv2.waitKey(1)\par
1746 \par
1747         mean_error = 0\par
1748         total_error = 0\par
1749         {\cf19 for} i {\cf19 in} range(len(objpoints)):\par
1750             imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)\par
1751             error = cv2.norm(imgpoints[i],imgpoints2, cv2.NORM_L2)/len(imgpoints2)\par
1752             total_error += error\par
1753         mean_error = total_error/len(objpoints)\par
1754 \par
1755         print(mean_error)\par
1756         cv2.destroyAllWindows()\par
1757 \par
1758         {\cf20 # calculate K & D}\par
1759         N_imm = len(checkerboard_images)\par
1760         K = np.zeros((3, 3))\par
1761         D = np.zeros((4, 1))\par
1762         rvecs = [np.zeros((1, 1, 3), dtype=np.float64) {\cf19 for} i {\cf19 in} range(N_imm)]\par
1763         tvecs = [np.zeros((1, 1, 3), dtype=np.float64) {\cf19 for} i {\cf19 in} range(N_imm)]\par
1764         retval, K, D, rvecs, tvecs = cv2.fisheye.calibrate(objpoints, imgpoints, imgshape[::-1], K, D, flags=calibration_flags, criteria=criteria)\par
1765         self.camera_matrix = K\par
1766         self.distioriton_matrix = D\par
1767         self.rotation_vector = rvecs\par
1768         self.translation_vector = tvecs\par
1769 \par
1770         {\cf19 if} out :\par
1771             {\cf17 with} open(out, {\cf22 'wb'}) {\cf17 as} f:\par
1772                 pickle.dump(K, f)\par
1773                 pickle.dump(D, f)\par
1774                 pickle.dump(rvecs, f)\par
1775                 pickle.dump(tvecs, f)\par
1776                 pickle.dump(error, f)\par
1777                 f.close()\par
1778 \par
1779                 \par
1780 \par
}
}
{\xe \v get_3d_point\:room_estimation}
{\xe \v room_estimation\:get_3d_point}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_3d_point (  {\i self},   {\i pixel},   {\i height} = {\f2 1500},   {\i camera_matrix} = {\f2 None},   {\i rotation_matrix} = {\f2 None},   {\i tvec} = {\f2 None},   {\i show_point} = {\f2 True})}}
\par
{\bkmkstart AAAAAAAAQN}
{\bkmkend AAAAAAAAQN}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 730     {\cf17 def }get_3d_point(self, pixel, height=1500, camera_matrix=None, rotation_matrix=None, tvec=None, show_point=True):\par
731 \par
732         {\cf19 if} {\cf19 not} camera_matrix:\par
733             camera_matrix = self.camera_matrix\par
734         \par
735         {\cf19 if} {\cf19 not} rotation_matrix:\par
736             rotation_matrix = self.camera_matrix\par
737 \par
738         {\cf19 if} {\cf19 not} tvec:\par
739             tvec = self.translation_vector\par
740         \par
741         {\cf19 if} show_point:\par
742             cv2.circle(self.show_img, pixel, 1, (255,255,0))\par
743             cv2.imshow(self.window_name, self.show_img)\par
744             cv2.waitKey(0)\par
745 \par
746         uv_point = np.array([pixel[0], pixel[1], 1])\par
747         left_side = np.linalg.inv(rotation_matrix * np.identity(3) ) * np.linalg.inv(camera_matrix * np.identity(3) ) * uv_point\par
748         right_side = np.linalg.inv(rotation_matrix * np.identity(3)) * tvec\par
749 \par
750         s = height + right_side[2][2]/left_side[2][2]\par
751 \par
752         point = np.linalg.inv(rotation_matrix * np.identity(3)) * (s*np.linalg.inv(camera_matrix* np.identity(3)) * uv_point - tvec)\par
753         {\cf19 return} point\par
754 \par
}
}
{\xe \v get_3d_to_2d\:room_estimation}
{\xe \v room_estimation\:get_3d_to_2d}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_3d_to_2d (  {\i self},   {\i point},   {\i show} = {\f2 True})}}
\par
{\bkmkstart AAAAAAAAQM}
{\bkmkend AAAAAAAAQM}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Point = (X, Z, Y) in mm (NOT PIXELS)\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 671     {\cf17 def }get_3d_to_2d(self, point, show=True):\par
672         {\cf22 '''}\par
673 {\cf22         Point = (X, Z, Y) in mm (NOT PIXELS)}\par
674 {\cf22         '''}\par
675         {\cf20 # np.ndarray((len(points), 3))}\par
676         {\cf20 # test = np.array(point[0], point[1], point[2], point[0], point[1], point[2], point[0], point[1], point[2])}\par
677         {\cf20 # points = np.float32(np.ndarray(points))}\par
678         point = np.float32([[point[0], point[1], point[2]]])\par
679         {\cf20 # print(point.shape)}\par
680         mapped_point, jac = cv2.projectPoints(point, self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
681         mapped_point = (int(mapped_point[0][0][0]),int(mapped_point[0][0][1]))\par
682 \par
683         {\cf19 if} show {\cf19 is} {\cf17 True}:\par
684             self.draw_axis()\par
685             cv2.circle(self.show_img, mapped_point, 6, (0, 255, 255))\par
686             cv2.imshow(self.window_name, self.show_img)\par
687             cv2.waitKey(0)\par
688 \par
689         {\cf19 return} mapped_point\par
690 \par
691 \par
692 \par
}
{
\ql
References room_estimation.axis, room_estimation.camera_matrix, room_estimation.corners, room_estimation.distioriton_matrix, room_estimation.draw_axis(), room_estimation.rotation_vector, room_estimation.show_img, room_estimation.translation_vector, and room_estimation.window_name.}\par
}
{\xe \v get_calibration_ratio\:room_estimation}
{\xe \v room_estimation\:get_calibration_ratio}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_calibration_ratio (  {\i self},   {\i video_resolution},   {\i calibration_resolution})}}
\par
{\bkmkstart AAAAAAAARS}
{\bkmkend AAAAAAAARS}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1948     {\cf17 def }get_calibration_ratio(self, video_resolution, calibration_resolution):\par
1949         w_ratio = video_resolution[0]/calibration_resolution[0]\par
1950         h_ratio = video_resolution[1]/calibration_resolution[1]\par
1951         {\cf19 return} w_ratio, h_ratio\par
1952 \par
}
}
{\xe \v get_checkerboard_shape\:room_estimation}
{\xe \v room_estimation\:get_checkerboard_shape}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_checkerboard_shape (  {\i self},   {\i checkerboard_images})}}
\par
{\bkmkstart AAAAAAAARM}
{\bkmkend AAAAAAAARM}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1679     {\cf17 def }get_checkerboard_shape(self, checkerboard_images):\par
1680         print({\cf22 "detecting checkerboard shape..."})\par
1681         found = {\cf17 False}\par
1682         {\cf19 for} i {\cf19 in} range(4,10):\par
1683             \par
1684             {\cf19 if} found:\par
1685                 {\cf19 break}\par
1686             {\cf19 for} j {\cf19 in} range(4,10):\par
1687                 print(i,j)\par
1688                 {\cf19 try}:\par
1689                     retval, corners = cv2.findChessboardCorners(checkerboard_images[0], (i,j))\par
1690                     found = retval\par
1691                 {\cf19 except}:\par
1692                     found={\cf17 False}\par
1693                 {\cf19 if} found:\par
1694                     checkerboard_grid = (i,j)\par
1695                     {\cf20 # break}\par
1696         print({\cf22 "Checkerboard shape:"}, checkerboard_grid)\par
1697         {\cf19 return} checkerboard_grid\par
1698 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARL" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.find_wall_intersection()}}}
.}\par
}
{\xe \v get_corner_with_definition\:room_estimation}
{\xe \v room_estimation\:get_corner_with_definition}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_corner_with_definition (  {\i self},   {\i event},   {\i x},   {\i y},   {\i flags},   {\i param})}}
\par
{\bkmkstart AAAAAAAAQH}
{\bkmkend AAAAAAAAQH}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 536     {\cf17 def }get_corner_with_definition(self, event, x, y, flags, param):\par
537         {\cf19 if} event == cv2.EVENT_LBUTTONDOWN:\par
538             self.corners.append((x,y))\par
539             cv2.circle(self.show_img, (x,y), 1,(0,0,255), 5, 0)\par
540             \par
541             width = input({\cf22 "Width"})\par
542             length = input({\cf22 "Length"})\par
543             self.room_points.append((int(width), int(length)))\par
544             self.connect_points(self.corners)\par
545 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARA" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.assign_non_square_corners()}}}
.}\par
}
{\xe \v get_corners\:room_estimation}
{\xe \v room_estimation\:get_corners}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_corners (  {\i self},   {\i event},   {\i x},   {\i y},   {\i flags},   {\i param})}}
\par
{\bkmkstart AAAAAAAAQG}
{\bkmkend AAAAAAAAQG}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 525     {\cf17 def }get_corners(self, event, x, y, flags, param):\par
526         {\cf20 # grab references to the global variables}\par
527         {\cf20 # global refPt, cropping, corner_count, corner_np}\par
528         {\cf20 # if the left mouse button was clicked, record the starting}\par
529         {\cf20 # (x, y) coordinates and indicate that cropping is being}\par
530         {\cf20 # performed}\par
531         {\cf19 if} event == cv2.EVENT_LBUTTONDOWN:\par
532             self.corners.append((x,y))\par
533             cv2.circle(self.show_img, (x,y), 1,(0,0,255), 5, 0)\par
534             print({\cf22 "CORNER !!!"})\par
535 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARA" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.assign_non_square_corners()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQD" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.get_room_corners()}}}
.}\par
}
{\xe \v get_cube_side\:room_estimation}
{\xe \v room_estimation\:get_cube_side}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_cube_side (  {\i self},   {\i cube},   {\i side})}}
\par
{\bkmkstart AAAAAAAAQI}
{\bkmkend AAAAAAAAQI}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Returns the point set of each side of the cube in \par
Start at bottom left of the plane on the opened cube displayed, in clockwise order  (or, the perspective from the center)\par
This function is to be paired before crop_plain() and the points returned from requested side should be passed on\par
Example: Side 1 contains points 1,2,6,5\par
\par
                                    |   5   |   \par
   5        6                       |_______|\par
   *--------*                       |       |\par
 / |       /|                       |   4   |\par
1 *---------*2|                _______|_______|_______\par
|  * 4    | *7              |       |       |       |\par
| /       |/                |   1   |   2   |   3   |\par
0 *---------*3                |_______|_______|_______|\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 546     {\cf17 def }get_cube_side(self, cube, side):\par
547         {\cf22 '''                      }\par
548 {\cf22 }\par
549 {\cf22         Returns the point set of each side of the cube in }\par
550 {\cf22         Start at bottom left of the plane on the opened cube displayed, in clockwise order  (or, the perspective from the center)}\par
551 {\cf22         This function is to be paired before crop_plain() and the points returned from requested side should be passed on}\par
552 {\cf22         Example: Side 1 contains points 1,2,6,5}\par
553 {\cf22 }\par
554 {\cf22                                             |   5   |   }\par
555 {\cf22            5        6                       |_______|}\par
556 {\cf22            *--------*                       |       |}\par
557 {\cf22          / |       /|                       |   4   |}\par
558 {\cf22       1 *---------*2|                _______|_______|_______}\par
559 {\cf22         |  * 4    | *7              |       |       |       |}\par
560 {\cf22         | /       |/                |   1   |   2   |   3   |}\par
561 {\cf22       0 *---------*3                |_______|_______|_______|}\par
562 {\cf22         '''}\par
563         points = {\cf18 None}\par
564 \par
565         {\cf19 if} side == 1: {\cf20 # Left Wall}\par
566             points = [cube[0], cube[1], cube[5], cube[4]]\par
567         {\cf19 elif} side == 2: {\cf20 # Floor}\par
568             points = [cube[0], cube[4], cube[7], cube[3]]\par
569         {\cf19 elif} side == 3: {\cf20 # Right Wall}\par
570             points = [cube[7], cube[6], cube[2], cube[3]]\par
571         {\cf19 elif} side == 4: {\cf20 # Back wall}\par
572             points = [cube[4], cube[5], cube[6], cube[7]]\par
573         {\cf19 elif} side == 5: {\cf20 # Ceiling}\par
574             points = [cube[5], cube[1], cube[2], cube[6]]\par
575         print(points)\par
576         print(np.array(points))\par
577         \par
578         {\cf19 return} points\par
579 \par
}
}
{\xe \v get_depth\:room_estimation}
{\xe \v room_estimation\:get_depth}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_depth (  {\i self},   {\i event},   {\i x},   {\i y},   {\i p1},   {\i p2})}}
\par
{\bkmkstart AAAAAAAARD}
{\bkmkend AAAAAAAARD}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid get z position given x and y.\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1419     {\cf17 def }get_depth(self, event, x, y, p1, p2):\par
1420         {\cf22 """}\par
1421 {\cf22         get z position given x and y.}\par
1422 {\cf22         """}\par
1423         {\cf20 # if event == cv2.EVENT_LBUTTONDOWN:}\par
1424         {\cf20 # resets the image so display does not accumulate}\par
1425         self.show_img = self.img_copy()\par
1426         min = np.amin(np.array(self.mapped[(x,y)]), 0)\par
1427         max = np.amax(np.array(self.mapped[(x,y)]), 0)\par
1428         self.show_img = cv2.putText(self.show_img, str((min, max)), (x,y), fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=1, color=(255,0,0), thickness=2)\par
1429         \par
1430         print(x,y, {\cf22 "->"}, min[:2], max[:2])\par
1431 \par
}
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARE" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.estimate()}}}
.}\par
}
{\xe \v get_files_from_folder\:room_estimation}
{\xe \v room_estimation\:get_files_from_folder}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_files_from_folder (  {\i self},   {\i folder},   {\i extension} = {\f2 "MP4"})}}
\par
{\bkmkstart AAAAAAAASH}
{\bkmkend AAAAAAAASH}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2734     {\cf17 def }get_files_from_folder(self, folder, extension="MP4"):\par
2735         \par
2736         files = []\par
2737         {\cf19 for} file {\cf19 in} os.listdir(folder):\par
2738             {\cf20 # check only text files}\par
2739             {\cf19 if} file.endswith(({\cf22 '.'} + extension)):\par
2740                 files.append((folder + file))\par
2741         {\cf19 return} files\par
2742 \par
}
}
{\xe \v get_n_room_points\:room_estimation}
{\xe \v room_estimation\:get_n_room_points}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_n_room_points (  {\i self},   {\i n_locations} = {\f2 None},   {\i height} = {\f2 0})}}
\par
{\bkmkstart AAAAAAAAQZ}
{\bkmkend AAAAAAAAQZ}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid N_locations are known locations along the floor (Non-square rooms)\par
    5000mm\par
--------------  \par
       3-----4  \par
       |     |\par
       |     |\par
1------2     | 5000mm  (Length)\par
|            |\par
|            |\par
0------------5\par
    (Width)\par
Where (width,length) in mm\par
0 = (0,0)\par
1 = (0, 2500)\par
2 = (2500, 2500)\par
3 = (2500, 5000)\par
4 = (5000, 5000)\par
5 = (5000, 0)\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1074     {\cf17 def }get_n_room_points(self, n_locations=None, height=0):\par
1075         {\cf22 '''}\par
1076 {\cf22         N_locations are known locations along the floor (Non-square rooms)}\par
1077 {\cf22             5000mm}\par
1078 {\cf22         --------------  }\par
1079 {\cf22                3-----4  }\par
1080 {\cf22                |     |}\par
1081 {\cf22                |     |}\par
1082 {\cf22         1------2     | 5000mm  (Length)}\par
1083 {\cf22         |            |}\par
1084 {\cf22         |            |}\par
1085 {\cf22         0------------5}\par
1086 {\cf22             (Width)}\par
1087 {\cf22         Where (width,length) in mm}\par
1088 {\cf22         0 = (0,0)}\par
1089 {\cf22         1 = (0, 2500)}\par
1090 {\cf22         2 = (2500, 2500)}\par
1091 {\cf22         3 = (2500, 5000)}\par
1092 {\cf22         4 = (5000, 5000)}\par
1093 {\cf22         5 = (5000, 0)}\par
1094 {\cf22         '''}\par
1095         {\cf19 if} n_locations == {\cf18 None}:\par
1096             n_locations = self.room_points\par
1097         {\cf20 # objectPoints = np.reshape(np.asfarray(n_locations), (len(n_locations),3,1))}\par
1098         objectPoints = np.ndarray((len(n_locations), 3, 1))\par
1099         {\cf19 for} index, location {\cf19 in} enumerate(n_locations):\par
1100             {\cf20 # for index2, dim in enumerate(location):}\par
1101             print(location)\par
1102             objectPoints[index][0][0] = location[0] {\cf20 # X }\par
1103             objectPoints[index][1][0] = location[1] {\cf20 # Z}\par
1104             objectPoints[index][2][0] = height      {\cf20 # Y}\par
1105             {\cf20 # print(objectPoints[index][:])}\par
1106         print(objectPoints)\par
1107         {\cf19 return} objectPoints\par
1108 \par
}
{
\ql
References room_estimation.room_points.}\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARA" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.assign_non_square_corners()}}}
.}\par
}
{\xe \v get_pixel_location\:room_estimation}
{\xe \v room_estimation\:get_pixel_location}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_pixel_location (  {\i self},   {\i pixel})}}
\par
{\bkmkstart AAAAAAAARF}
{\bkmkend AAAAAAAARF}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Returns the pixel location in 3D\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1452     {\cf17 def }get_pixel_location(self, pixel):\par
1453         {\cf22 '''}\par
1454 {\cf22         Returns the pixel location in 3D}\par
1455 {\cf22         '''}\par
1456         {\cf19 return} self.mapped[pixel]\par
1457 \par
1458 \par
}
{
\ql
References room_estimation.mapped.}\par
}
{\xe \v get_room_3d\:room_estimation}
{\xe \v room_estimation\:get_room_3d}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_room_3d (  {\i self},   {\i limits},   {\i height},   {\i step},   {\i save_filename} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAAQV}
{\bkmkend AAAAAAAAQV}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1010     {\cf17 def }get_room_3d(self, limits, height, step, save_filename=None):\par
1011         min, max = limits\par
1012         \par
1013         points = np.mgrid[min[0]:max[0]:step, min[1]:max[1]:step, height:height+1:1].reshape(3,-1).T.astype(np.float) {\cf20 # all 40 inches}\par
1014         print({\cf22 "Projecting points with size"}, points.shape)\par
1015         mapped_pixels, jac = cv2.projectPoints(points, self.rotation_vector, self.translation_vector, self.camera_matrix, self.distioriton_matrix)\par
1016 \par
1017         print({\cf22 "Mapping..."})\par
1018         mapped = \{\}\par
1019         {\cf19 for} index, pixel {\cf19 in} enumerate(mapped_pixels):\par
1020             \par
1021             point = points[index]\par
1022             pixel = (int(pixel[0][0]), int(pixel[0][1]))\par
1023             {\cf20 # image = cv2.circle(image, pixel, 1, (1,1,1))}\par
1024             {\cf19 if} pixel {\cf19 in} mapped.keys():\par
1025                 mapped[pixel].append(point)\par
1026             {\cf19 else}:\par
1027                 mapped[pixel] = [point]\par
1028         self.mapped = mapped\par
1029         print({\cf22 "Mapping Complete."})\par
1030         {\cf19 return} mapped\par
1031 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARE" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.estimate()}}}
.}\par
}
{\xe \v get_room_corners\:room_estimation}
{\xe \v room_estimation\:get_room_corners}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_room_corners (  {\i self},   {\i num_points} = {\f2 4})}}
\par
{\bkmkstart AAAAAAAAQD}
{\bkmkend AAAAAAAAQD}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Assigns 2D Room corners which will map to the real world 3D.\par
\par
This function is a loop for the number of room points (real world) and lets you select the location they should exist in the video.\par
\par
Each point should be respective to the order that room points exist. For example, consistently clockwise starting from the bottom left.\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 467     {\cf17 def }get_room_corners(self, num_points=4):\par
468         {\cf22 '''}\par
469 {\cf22         Assigns 2D Room corners which will map to the real world 3D.}\par
470 {\cf22 }\par
471 {\cf22         This function is a loop for the number of room points (real world) and lets you select the location they should exist in the video.}\par
472 {\cf22 }\par
473 {\cf22         Each point should be respective to the order that room points exist. For example, consistently clockwise starting from the bottom left.}\par
474 {\cf22         '''}\par
475         self.clear_corners()\par
476         print({\cf22 "Assigning corners..."})\par
477 \par
478         {\cf20 #Initializes feedback to a function}\par
479         cv2.setMouseCallback({\cf22 "Room Estimation"}, self.get_corners)\par
480         print({\cf22 "Setting Callback"})\par
481         {\cf19 while} len(self.corners) < num_points:\par
482 \par
483             {\cf20 # display the image and wait for a keypress}\par
484             cv2.imshow({\cf22 "Room Estimation"}, self.show_img)\par
485             cv2.setMouseCallback({\cf22 "Room Estimation"}, self.get_corners)\par
486             key = cv2.waitKey(1) & 0xFF\par
487             self.connect_points(self.corners)\par
488 \par
489             {\cf20 # if len(self.corners) >= 4:}\par
490             {\cf20 #     self.show_img, cube, target_cube = self.draw_cuboid()}\par
491 \par
492             {\cf19 if} key == ord({\cf22 "c"}):\par
493                 {\cf19 return} self.connect_points(self.corners)\par
494             \par
495         self.connect_points(self.corners)\par
496     \par
497         self.corners = np.reshape(np.asfarray(self.corners), (num_points,2,1))\par
498         {\cf20 # cv2.setMouseCallback("Room Estimation", self.draw_vector)}\par
499         {\cf19 return} self.corners\par
500         \par
}
{
\ql
References room_estimation.clear_corners(), room_estimation.connect_points(), room_estimation.corners, room_estimation.get_corners, room_estimation.get_corners(), room_estimation.room_points, room_estimation.show_img, and room_estimation.window_name.}\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARK" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.define_perspective_grid()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARL" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.find_wall_intersection()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARJ" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.superimpose_checker()}}}
.}\par
}
{\xe \v get_room_dimensions\:room_estimation}
{\xe \v room_estimation\:get_room_dimensions}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_room_dimensions (  {\i self},   {\i room_points})}}
\par
{\bkmkstart AAAAAAAASB}
{\bkmkend AAAAAAAASB}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Given room points returns the width and length of the room given the farthest points.\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2307     {\cf17 def }get_room_dimensions(self, room_points):\par
2308         {\cf22 '''}\par
2309 {\cf22         Given room points returns the width and length of the room given the farthest points.}\par
2310 {\cf22         '''}\par
2311 \par
2312         max_width = 0\par
2313         max_length = 0\par
2314         \par
2315         {\cf19 for} point {\cf19 in} room_points:\par
2316             x, z, y = point\par
2317             {\cf19 if} x > max_width:\par
2318                 max_width = x\par
2319             {\cf19 if} z > max_length:\par
2320                 max_length = z\par
2321 \par
2322         {\cf19 return} max_width, max_length\par
2323 \par
}
}
{\xe \v get_room_points\:room_estimation}
{\xe \v room_estimation\:get_room_points}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
get_room_points (  {\i self},   {\i width},   {\i length},   {\i height} = {\f2 0})}}
\par
{\bkmkstart AAAAAAAAQY}
{\bkmkend AAAAAAAAQY}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1046     {\cf17 def }get_room_points(self, width, length, height=0):\par
1047         print({\cf22 "Creating room points"})\par
1048         {\cf22 """}\par
1049 {\cf22         Returns a matrix for real world points.}\par
1050 {\cf22         These points start on the bottom left and are assigned as a rectangle in a clockwise direction}\par
1051 {\cf22         }\par
1052 {\cf22         (width)}\par
1053 {\cf22         1------2 }\par
1054 {\cf22         |      |}\par
1055 {\cf22         |      | (length)}\par
1056 {\cf22         0      3}\par
1057 {\cf22 }\par
1058 {\cf22         We may include Height, but this is not included.}\par
1059 {\cf22 }\par
1060 {\cf22         Format of points (X,Z,Y)}\par
1061 {\cf22         """}\par
1062 \par
1063         objectPoints = np.array(\par
1064             [\par
1065                 [[0.0],[0.0],[float(height)]],\par
1066                 [[0.0],[float(length)],[float(height)]],\par
1067                 [[float(width)],[float(length)],[float(height)]],\par
1068                 [[float(width)],[0.0],[float(height)]],\par
1069             ]\par
1070         ) \par
1071         print(objectPoints.shape)\par
1072         {\cf19 return} objectPoints\par
1073     \par
}
}
{\xe \v img_copy\:room_estimation}
{\xe \v room_estimation\:img_copy}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
img_copy (  {\i self})}}
\par
{\bkmkstart AAAAAAAAPZ}
{\bkmkend AAAAAAAAPZ}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 242     {\cf17 def }img_copy(self):\par
243         {\cf19 return} deepcopy(self.image)\par
244 \par
}
}
{\xe \v load_and_undistort_calibration\:room_estimation}
{\xe \v room_estimation\:load_and_undistort_calibration}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
load_and_undistort_calibration (  {\i self},   {\i video_path},   {\i visualize_distortion} = {\f2 False},   {\i points_data} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAASF}
{\bkmkend AAAAAAAASF}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2478     {\cf17 def }load_and_undistort_calibration(self, video_path, visualize_distortion=False, points_data=None):\par
2479         calibration_path = video_path[:-3] + {\cf22 "pickle"}\par
2480         checkerboard_image = self.collect_frames(video_path,1,1,2)[0]\par
2481 \par
2482         self.calibration_resolution = (checkerboard_image.shape[1], checkerboard_image.shape[0])\par
2483 \par
2484         c, d, r, t, error, resolution = self.load_calibration(calibration_path)\par
2485         checkerboard = self.undistort_room(image=checkerboard_image, points_data=points_data)\par
2486         visroom = self.undistort_room()\par
2487         cv2.imshow(video_path, visroom)\par
2488         cv2.imshow({\cf22 "checkerboard"}, checkerboard)\par
2489 \par
2490         {\cf19 if} visualize_distortion:\par
2491             self.visualize_distortion(c, d)\par
2492 \par
2493         cv2.waitKey(0)\par
2494         {\cf19 return} visroom, checkerboard, c, d ,r ,t, error, resolution\par
2495 \par
}
}
{\xe \v load_calibration\:room_estimation}
{\xe \v room_estimation\:load_calibration}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
load_calibration (  {\i self},   {\i file})}}
\par
{\bkmkstart AAAAAAAARQ}
{\bkmkend AAAAAAAARQ}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Loads in order\par
1 Camera matrix\par
2 Distortion Matrix\par
3 Rvecs\par
4 Tvecs\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1866     {\cf17 def }load_calibration(self, file):\par
1867         {\cf22 """}\par
1868 {\cf22         Loads in order}\par
1869 {\cf22         1 Camera matrix}\par
1870 {\cf22         2 Distortion Matrix}\par
1871 {\cf22         3 Rvecs}\par
1872 {\cf22         4 Tvecs}\par
1873 {\cf22         """}\par
1874 \par
1875         {\cf17 with} open(file, {\cf22 'rb'}) {\cf17 as} f:\par
1876             mtx = pickle.load(f)\par
1877             dist =pickle.load(f)\par
1878             rvecs =pickle.load(f)\par
1879             tvecs =pickle.load(f)\par
1880             error_list = pickle.load(f)\par
1881             calibration_resolution = pickle.load(f)\par
1882             f.close()\par
1883 \par
1884         print(mtx)\par
1885         self.camera_matrix = mtx\par
1886         self.distioriton_matrix = dist\par
1887         self.rotation_vector = rvecs\par
1888         self.translation_vector = tvecs\par
1889         self.error_list = error_list\par
1890         self.calibration_resolution\par
1891         {\cf19 return} mtx, dist, rvecs, tvecs, error_list, calibration_resolution\par
1892 \par
}
{
\ql
References room_estimation.calibration_resolution, room_estimation.camera_matrix, room_estimation.distioriton_matrix, room_estimation.error_list, room_estimation.rotation_vector, and room_estimation.translation_vector.}\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABQ" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.save_room()}}}
.}\par
}
{\xe \v load_room\:room_estimation}
{\xe \v room_estimation\:load_room}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
load_room (  {\i self},   {\i filename})}}
\par
{\bkmkstart AAAAAAAASE}
{\bkmkend AAAAAAAASE}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2459     {\cf17 def }load_room(self, filename):\par
2460         {\cf19 if} filename :\par
2461 \par
2462             {\cf17 with} open(filename, {\cf22 'wb'}) {\cf17 as} f:\par
2463                 self.room_points = pickle.load(f)\par
2464                 self.corners = pickle.load(f)\par
2465                 self.room_width = pickle.load(f)\par
2466                 self.room_length = pickle.load(f)\par
2467                 self.room_height = pickle.load(f)\par
2468                 self.mapped = pickle.load(f)\par
2469                 self.camera_matrix = pickle.load(f)\par
2470                 self.distioriton_matrix = pickle.load(f)\par
2471                 self.rotation_vector = pickle.load(f)\par
2472                 self.translation_vector = pickle.load(f)\par
2473                 self.calibration_resolution = pickle.load(f)\par
2474 \par
2475 \par
2476                 f.close()\par
2477 \par
}
}
{\xe \v load_room_3d\:room_estimation}
{\xe \v room_estimation\:load_room_3d}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
load_room_3d (  {\i self},   {\i filename})}}
\par
{\bkmkstart AAAAAAAAQX}
{\bkmkend AAAAAAAAQX}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1040     {\cf17 def }load_room_3d(self, filename):\par
1041         {\cf17 with} open(filename, {\cf22 'rb'}) {\cf17 as} f:\par
1042             mapped = pickle.load(f)\par
1043             f.close()\par
1044         {\cf19 return} mapped\par
1045 \par
}
}
{\xe \v load_tracker\:room_estimation}
{\xe \v room_estimation\:load_tracker}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
load_tracker (  {\i self},   {\i csv_file})}}
\par
{\bkmkstart AAAAAAAARV}
{\bkmkend AAAAAAAARV}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Loads the tracker data of a csv file\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2100     {\cf17 def }load_tracker(self, csv_file):\par
2101         {\cf22 '''}\par
2102 {\cf22         Loads the tracker data of a csv file}\par
2103 {\cf22         '''}\par
2104 \par
2105         df = pd.read_csv(csv_file)\par
2106         fps = int(round(df.iloc[0][{\cf22 'FrameRate'}]))\par
2107         self.invert_y = int(df.iloc[1][{\cf22 'Max_Pixel_y'}])\par
2108 \par
2109         df = df[[{\cf22 'Frame_Num'},{\cf22 'Pixel_Loc_x'},{\cf22 'Pixel_Loc_y'}, {\cf22 'Name'}, {\cf22 'ID'},{\cf22 'Max_Pixel_x'}, {\cf22 'Max_Pixel_y'}, {\cf22 'Width(px)'}, {\cf22 'Height(px)'}]]\par
2110         df = self.correct_tracker_points(df)\par
2111         print(df)\par
2112         {\cf19 return} df\par
2113 \par
2114 \par
}
{
\ql
References {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARW" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.correct_tracker_points()}}}
, tracker_evaluation.invert_y, and room_estimation.invert_y.}\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABS" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.stitch_trackers()}}}
.}\par
}
{\xe \v make_interpolater\:room_estimation}
{\xe \v room_estimation\:make_interpolater}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
make_interpolater (  {\i left_min},   {\i left_max},   {\i right_min},   {\i right_max})}}
\par
{\bkmkstart AAAAAAAARI}
{\bkmkend AAAAAAAARI}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Depreciated.\par
Supposed to be used to divide a grid space given pixels.\par
Interpolates values in between min and max.\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1506     {\cf17 def }make_interpolater(left_min, left_max, right_min, right_max): \par
1507         {\cf22 '''}\par
1508 {\cf22         Depreciated.}\par
1509 {\cf22         Supposed to be used to divide a grid space given pixels.}\par
1510 {\cf22         Interpolates values in between min and max.}\par
1511 {\cf22         '''}\par
1512         {\cf20 # Figure out how 'wide' each range is  }\par
1513         leftSpan = left_max - left_min  \par
1514         rightSpan = right_max - right_min  \par
1515 \par
1516         {\cf20 # Compute the scale factor between left and right values }\par
1517         scaleFactor = float(rightSpan) / float(leftSpan) \par
1518 \par
1519         {\cf20 # create interpolation function using pre-calculated scaleFactor}\par
1520         {\cf17 def }interp_fn(value):\par
1521             {\cf19 return} right_min + (value-left_min)*scaleFactor\par
1522 \par
1523         {\cf19 return} interp_fn\par
1524 \par
}
}
{\xe \v plot_points\:room_estimation}
{\xe \v room_estimation\:plot_points}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
plot_points (  {\i event},   {\i x},   {\i y},   {\i flags},   {\i params})}}
\par
{\bkmkstart AAAAAAAAQL}
{\bkmkend AAAAAAAAQL}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 638     {\cf17 def }plot_points(event,x,y,flags,params):\par
639         {\cf17 global} mouseX,mouseY\par
640         {\cf20 # print(params)}\par
641         mapped_points = params\par
642         p_list = []\par
643         x_list = []\par
644         y_list = []\par
645         z_list = []\par
646         {\cf19 if} event == cv2.EVENT_LBUTTONDBLCLK:\par
647             {\cf19 if} (x,y) {\cf19 in} mapped_points.keys():\par
648                 p_list = mapped_points[(x,y)]\par
649                 \par
650                 \par
651                 {\cf19 for} p {\cf19 in} p_list:\par
652                     print(p)\par
653                     x_list.append(p[0])\par
654                     y_list.append(p[2])\par
655                     z_list.append(p[1])\par
656 \par
657 \par
658 \par
659                 fig = plt.figure(figsize=(4,4))\par
660                 ax = fig.add_subplot(111, projection={\cf22 '3d'})\par
661                 ax.scatter(xs = x_list, ys = y_list, zs = z_list)\par
662 \par
663                 ax.set_title({\cf22 "3D Point position in room"})\par
664 \par
665                 ax.set_xlabel({\cf22 "X (Width"})\par
666                 ax.set_ylabel({\cf22 "Y (Height)"})\par
667                 ax.set_zlabel({\cf22 "Z (Depth"})\par
668 \par
669                 plt.show()\par
670 \par
}
}
{\xe \v poly_fit_wall\:room_estimation}
{\xe \v room_estimation\:poly_fit_wall}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
poly_fit_wall (  {\i self},   {\i samples},   {\i pixels_past_extent} = {\f2 100},   {\i horizontal_extent} = {\f2 None},   {\i degree} = {\f2 2})}}
\par
{\bkmkstart AAAAAAAARG}
{\bkmkend AAAAAAAARG}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Samples exist in format np.array([x,y,z], [x,y,z])\par
\par
Used to find intersection points.\par
\par
Utilized to define points in the grid.\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1459     {\cf17 def }poly_fit_wall(self, samples, pixels_past_extent=100,horizontal_extent=None, degree=2):\par
1460         {\cf22 '''}\par
1461 {\cf22         Samples exist in format np.array([x,y,z], [x,y,z])}\par
1462 {\cf22 }\par
1463 {\cf22         Used to find intersection points.}\par
1464 {\cf22 }\par
1465 {\cf22         Utilized to define points in the grid.}\par
1466 {\cf22         '''}\par
1467 \par
1468         samples = np.reshape(samples, (samples.shape[0], 2))\par
1469         {\cf19 if} horizontal_extent {\cf19 is} {\cf18 None}:\par
1470             min = np.amin(samples, 0)[0]\par
1471             max = np.amax(samples, 0)[0]\par
1472             horizontal_extent = np.linspace(min  - pixels_past_extent, max + pixels_past_extent)\par
1473 \par
1474 \par
1475         {\cf20 # Polyfit a model on the samples x,y then create a line with the exten of the horizontal extent we provide}\par
1476         fit = np.polyfit(samples[:,0], samples[:,1], degree)\par
1477         \par
1478         polynomial_equation = np.poly1d(fit)\par
1479         print({\cf22 "Equation"}, polynomial_equation)\par
1480         \par
1481         pred_line = polynomial_equation(horizontal_extent)\par
1482         pred_line = np.reshape(pred_line, (len(pred_line),1))\par
1483         horizontal_extent = np.reshape(horizontal_extent, (len(horizontal_extent),1))\par
1484         {\cf20 # print(pred_line)}\par
1485         {\cf19 return} np.hstack((horizontal_extent, pred_line)), polynomial_equation\par
1486 \par
}
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARK" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.define_perspective_grid()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARL" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.find_wall_intersection()}}}
.}\par
}
{\xe \v project_point\:room_estimation}
{\xe \v room_estimation\:project_point}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
project_point (  {\i self},   {\i event},   {\i x},   {\i y},   {\i flags},   {\i params})}}
\par
{\bkmkstart AAAAAAAARC}
{\bkmkend AAAAAAAARC}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1416     {\cf17 def }project_point(self, event,x,y,flags,params):\par
1417         {\cf19 pass}\par
1418     \par
}
}
{\xe \v project_points\:room_estimation}
{\xe \v room_estimation\:project_points}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
project_points (  {\i self},   {\i points} = {\f2 []})}}
\par
{\bkmkstart AAAAAAAAQK}
{\bkmkend AAAAAAAAQK}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 632     {\cf17 def }project_points(self, points=[]):\par
633         points_np = np.reshape(np.asfarray(points), (len(points),2,1))\par
634 \par
635 \par
636 \par
}
}
{\xe \v save_calibration\:room_estimation}
{\xe \v room_estimation\:save_calibration}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
save_calibration (  {\i self},   {\i file},   {\i mtx},   {\i distortion},   {\i rvecs},   {\i tvecs},   {\i error_list},   {\i resolution})}}
\par
{\bkmkstart AAAAAAAARP}
{\bkmkend AAAAAAAARP}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1855     {\cf17 def }save_calibration(self, file, mtx, distortion, rvecs, tvecs, error_list, resolution):\par
1856         {\cf19 if} file :\par
1857             {\cf17 with} open(file, {\cf22 'wb'}) {\cf17 as} f:\par
1858                 pickle.dump(mtx, f)\par
1859                 pickle.dump(distortion, f)\par
1860                 pickle.dump(rvecs, f)\par
1861                 pickle.dump(tvecs, f)\par
1862                 pickle.dump(error_list, f)\par
1863                 pickle.dump(resolution, f)\par
1864                 f.close()\par
1865 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARL" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.find_wall_intersection()}}}
.}\par
}
{\xe \v save_room\:room_estimation}
{\xe \v room_estimation\:save_room}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
save_room (  {\i self},   {\i filename})}}
\par
{\bkmkstart AAAAAAAABQ}
{\bkmkend AAAAAAAABQ}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Saves all values which define a room.\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2428     {\cf17 def }save_room(self, filename):\par
2429         {\cf22 '''}\par
2430 {\cf22         Saves all values which define a room.}\par
2431 {\cf22         '''}\par
2432         \par
2433         {\cf19 if} filename :\par
2434 \par
2435             {\cf17 with} open(filename, {\cf22 'wb'}) {\cf17 as} f:\par
2436                 pickle.dump(self.room_points, f)\par
2437                 pickle.dump(self.corners, f)\par
2438 \par
2439                 {\cf20 # Room Dimensions}\par
2440                 pickle.dump(self.room_width, f)\par
2441                 pickle.dump(self.room_length, f)\par
2442                 pickle.dump(self.room_height, f)\par
2443                 \par
2444                 {\cf20 # Mapped coordinates from 2D to 3D. Useful for loading up tracker and getting 3D points}\par
2445                 pickle.dump(self.mapped, f)\par
2446 \par
2447                 {\cf20 # Save calibration.}\par
2448                 pickle.dump(self.camera_matrix, f)\par
2449                 pickle.dump(self.distioriton_matrix, f)\par
2450                 pickle.dump(self.rotation_vector, f)\par
2451                 pickle.dump(self.translation_vector, f)\par
2452 \par
2453                 pickle.dump(self.calibration_resolution, f)\par
2454                 f.close()\par
2455 \par
2456             {\cf20 # Saves camera calibration}\par
2457             {\cf20 # self.save_calibration(filename)}\par
2458 \par
}
{
\ql
References room_estimation.calibration_resolution, room_estimation.camera_matrix, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAAT" }{}}{\fldrslt {\cs37\ul\cf2 CameraPosition.collect_frames()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARR" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.collect_frames()}}}
, room_estimation.corners, room_estimation.distioriton_matrix, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARQ" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.load_calibration()}}}
, room_estimation.mapped, room_estimation.room_height, room_estimation.room_length, room_estimation.room_points, room_estimation.room_width, room_estimation.rotation_vector, room_estimation.translation_vector, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAART" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.undistort_room()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAASD" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.visualize_distortion()}}}
.}\par
}
{\xe \v save_room_3d\:room_estimation}
{\xe \v room_estimation\:save_room_3d}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
save_room_3d (  {\i self},   {\i path},   {\i filename},   {\i mapped})}}
\par
{\bkmkstart AAAAAAAAQW}
{\bkmkend AAAAAAAAQW}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1032     {\cf17 def }save_room_3d(self, path, filename, mapped):\par
1033         print({\cf22 "Saving mapped points..."})\par
1034         {\cf19 if} path {\cf19 is} {\cf19 not} {\cf18 None}:\par
1035             {\cf17 with} open(filename, {\cf22 'wb'}) {\cf17 as} f:\par
1036                 pickle.dump(mapped, f)\par
1037                 f.close()\par
1038         print({\cf22 "Save Complete."})\par
1039     \par
}
}
{\xe \v set_axis\:room_estimation}
{\xe \v room_estimation\:set_axis}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
set_axis (  {\i self},   {\i x},   {\i y},   {\i z})}}
\par
{\bkmkstart AAAAAAAAPX}
{\bkmkend AAAAAAAAPX}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 226     {\cf17 def }set_axis(self, x, y, z):\par
227         {\cf20 # axis = np.float32([[0,0,0], [0,3000,0], [3000,3000,0], [3000,0,0],}\par
228         {\cf20 #             [0,0,3000],[0,3000,3000],[3000,3000,3000],[3000,0,3000] ])}\par
229         self.axis = np.float32([[x,0,0], [0,z,0], [0,0,y]]).reshape(-1,3)\par
230         {\cf19 return} self.axis\par
231 \par
}
}
{\xe \v set_corners\:room_estimation}
{\xe \v room_estimation\:set_corners}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
set_corners (  {\i self},   {\i p1} = {\f2 None},   {\i p2} = {\f2 None},   {\i p3} = {\f2 None},   {\i p4} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAAPY}
{\bkmkend AAAAAAAAPY}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 232     {\cf17 def }set_corners(self, p1=None, p2=None, p3=None, p4=None):\par
233         {\cf19 if} p1:\par
234             self.corner[0] = np.asarray([p1[0],p1[1]]).reshape(1,2,1)\par
235         {\cf19 if} p2:\par
236             self.corner[1] = np.asarray([p2[0],p2[1]]).reshape(1,2,1)\par
237         {\cf19 if} p3:\par
238             self.corner[2] = np.asarray([p3[0],p3[1]]).reshape(1,2,1)\par
239         {\cf19 if} p3:\par
240             self.corner[3] = np.asarray([p4[0],p4[1]]).reshape(1,2,1)\par
241 \par
}
}
{\xe \v set_image\:room_estimation}
{\xe \v room_estimation\:set_image}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
set_image (  {\i self},   {\i image})}}
\par
{\bkmkstart AAAAAAAAPV}
{\bkmkend AAAAAAAAPV}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 198     {\cf17 def }set_image(self, image):\par
199         self.image = image\par
200 \par
}
}
{\xe \v show_tracker_2D\:room_estimation}
{\xe \v room_estimation\:show_tracker_2D}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
show_tracker_2D (  {\i self},   {\i df},   {\i image},   {\i data_columns} = {\f2 ['Pixel_Loc_x',\~ 'Pixel_Loc_y']},   {\i window} = {\f2 "Tracker2D"},   {\i colour} = {\f2 (255,255,0)})}}
\par
{\bkmkstart AAAAAAAARZ}
{\bkmkend AAAAAAAARZ}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2253     {\cf17 def }show_tracker_2D(self, df, image, data_columns =['Pixel_Loc_x', 'Pixel_Loc_y'], window="Tracker2D", colour = (255,255,0)):\par
2254         x = df[data_columns[0]].tolist()\par
2255         y = df[data_columns[1]].tolist()\par
2256 \par
2257         {\cf19 for} index {\cf19 in} range(len(x)):\par
2258             image = cv2.circle(image, (int(x[index]), int(y[index])), 2, colour, 2)\par
2259             cv2.imshow(window, image)\par
2260             cv2.waitKey(1)\par
2261 \par
2262 \par
}
}
{\xe \v show_tracker_3D\:room_estimation}
{\xe \v room_estimation\:show_tracker_3D}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
show_tracker_3D (  {\i self},   {\i df},   {\i mapped_min},   {\i mapped_max})}}
\par
{\bkmkstart AAAAAAAARY}
{\bkmkend AAAAAAAARY}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2145     {\cf17 def }show_tracker_3D(self, df, mapped_min, mapped_max):\par
2146         {\cf20 # https://stackoverflow.com/questions/38118598/3d-animation-using-matplotlib}\par
2147         {\cf20 # Convert df points numpy array}\par
2148         x_df = df[{\cf22 'Pixel_Loc_x'}]\par
2149         y_df = df[{\cf22 'Pixel_Loc_y'}]\par
2150 \par
2151 \par
2152 \par
2153         {\cf20 # N = 100}\par
2154         {\cf20 # data = np.array(list(gen(N))).T}\par
2155         {\cf20 # line, = ax.plot(data[0, 0:1], data[1, 0:1], data[2, 0:1])}\par
2156 \par
2157 \par
2158 \par
2159         x_list1 = []\par
2160         y_list1 = []\par
2161         z_list1 = []\par
2162 \par
2163         x_list2 = []\par
2164         y_list2 = []\par
2165         z_list2 = []\par
2166         {\cf19 for} index {\cf19 in} range(len(x_df)):\par
2167             point = (int(x_df.loc[index+1]), int(y_df.loc[index+1]))\par
2168 \par
2169             {\cf20 # Interactive mode}\par
2170 \par
2171             {\cf19 if} point {\cf19 in} mapped_min.keys():\par
2172 {\cf20 # }\par
2173                 min_point = mapped_min[point]\par
2174                 {\cf20 # for p in min_point:}\par
2175                     {\cf20 # x,y,z = p}\par
2176                 x,z,y = min_point[0]\par
2177                 x_list1.append(x)\par
2178                 y_list1.append(y)\par
2179                 z_list1.append(z)\par
2180                 {\cf20 # scatter = ax.scatter3D(x, y,z, cmap='Greens')}\par
2181                 {\cf20 # self.front_plt = ax.scatter3D(x, y, z)}\par
2182 \par
2183             {\cf19 if} point {\cf19 in} mapped_max.keys():\par
2184                 max_point = mapped_max[point]\par
2185 \par
2186                 {\cf20 # for p in max_point:}\par
2187                 x,z,y = max_point[0]\par
2188                 x_list2.append(x)\par
2189                 y_list2.append(y)\par
2190                 z_list2.append(z)\par
2191         plt.ion\par
2192         {\cf19 if} self.fig {\cf19 is} {\cf18 None}:\par
2193             self.fig = plt.figure(1)\par
2194             self.ax = plt.axes(projection={\cf22 '3d'})\par
2195 \par
2196 \par
2197             self.back_plt = self.ax.scatter3D(x_list1, y_list1, z_list1, cmap={\cf22 'Greens'})\par
2198             self.back_plt = self.ax.scatter3D(x_list2, y_list2, z_list2, cmap={\cf22 'Blues'})\par
2199 \par
2200             {\cf20 # self.line_plt = ax.plot3D([front_point[0],back_point[0]], [front_point[1],back_point[1]], [0,self.room_length], color='teal')}\par
2201             \par
2202             {\cf20 # plt.quiver(*origin, V[:,0], V[:,1], color=['r','b','g'], scale=21)}\par
2203             self.ax.set_xlim(-self.room_width,self.room_width)\par
2204             self.ax.set_ylim(-self.room_height,self.room_height)\par
2205             self.ax.set_zlim(-self.room_length,self.room_length)\par
2206             self.fig.canvas.draw()\par
2207             self.fig.canvas.flush_events()\par
2208             self.fig.show()\par
2209 \par
2210             {\cf20 # else:}\par
2211                 {\cf20 # self.ax.cla()}\par
2212                 {\cf20 # if values:}\par
2213                 {\cf20 #     self.points_plt = self.ax.scatter3D(x_list,y_list,z_list, marker="D", color="red")}\par
2214                 {\cf20 # # plt.quiver((0,0,0), (0,1,0), , color=['r','b','g'], scale=21)}\par
2215                 {\cf20 # self.ax.set_xlim(-self.room_width,self.room_width)}\par
2216                 {\cf20 # self.ax.set_ylim(-self.room_height,self.room_width)}\par
2217                 {\cf20 # self.ax.set_zlim(-self.room_length,self.room_width)}\par
2218                 {\cf20 # self.ax.set_xlabel("X")}\par
2219                 {\cf20 # self.ax.set_ylabel("Y")}\par
2220                 {\cf20 # self.ax.set_zlabel("Z")}\par
2221                 \par
2222                 {\cf20 # self.float_plt = self.ax.scatter3D(floater_point[0],floater_point[1],self.vector_depth, marker="o", color="blue")}\par
2223                 {\cf20 # self.front_plt = self.ax.scatter3D(front_point[0], front_point[1], 0, marker=("o"), color="cyan")}\par
2224                 {\cf20 # self.back_plt = self.ax.scatter3D(back_point[0],back_point[1], self.room_length, marker="x", color="cyan")}\par
2225                 {\cf20 # self.line_plt = self.ax.plot3D([front_point[0],back_point[0]], [front_point[1],back_point[1]], [0,self.room_length], color='teal')}\par
2226                 {\cf20 # self.quiver = self.ax.quiver([0,0,0], [0,0,0], [0,0,0], [self.room_width,0,0], [0,self.room_height,0], [0,0,self.room_length], length=0.1, normalize=False)}\par
2227                 {\cf20 # self.fig.canvas.draw()}\par
2228                 {\cf20 # self.fig.canvas.flush_events()}\par
2229             {\cf20 # image = cv2.circle(image, point, 2, (255,255,0), 2)}\par
2230             {\cf20 # cv2.imshow("Tracker2D", image)}\par
2231             {\cf20 # cv2.waitKey(1)}\par
2232 \par
2233             {\cf20 # if point in mapped_min.keys():}\par
2234             {\cf20 #     min_point = mapped_min[point]}\par
2235 \par
2236             {\cf20 #     for p in min_point:}\par
2237             {\cf20 #         x,y,z = p}\par
2238             {\cf20 #         scatter = ax.scatter3D(x, y,z, cmap='Greens')}\par
2239             {\cf20 #         # ax.plot_surface(x, y, z, color=('r'))}\par
2240 \par
2241             {\cf20 # if point in mapped_max.keys():}\par
2242             {\cf20 #     max_point = mapped_max[point]}\par
2243 \par
2244             {\cf20 #     for p in max_point:}\par
2245             {\cf20 #         x,y,z = p}\par
2246             {\cf20 #         scatter = ax.scatter3D(x, y,z, cmap='Blues')}\par
2247 \par
2248             \par
2249             \par
2250         {\cf19 return} self.show_img\par
2251 \par
2252 \par
}
}
{\xe \v stitch_rooms\:room_estimation}
{\xe \v room_estimation\:stitch_rooms}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
stitch_rooms (  {\i self},   {\i video_list},   {\i undistort} = {\f2 True},   {\i key_index} = {\f2 None},   {\i show} = {\f2 False})}}
\par
{\bkmkstart AAAAAAAABR}
{\bkmkend AAAAAAAABR}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Stitches rooms with custom keypoints with SIFT descriptors\par
\par
undistort uses current room undistortion parameters to undistort all the videos in video_list\par
\par
key_index is the index for which video is the source that all others will use as alignment reference.\par
\par
returns a stitched image that is overlapped by all videos and a list of homography transformation matrices for every video index\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2506     {\cf17 def }stitch_rooms(self, video_list, undistort=True, key_index=None, show=False):\par
2507         {\cf22 '''}\par
2508 {\cf22         Stitches rooms with custom keypoints with SIFT descriptors}\par
2509 {\cf22 }\par
2510 {\cf22         undistort uses current room undistortion parameters to undistort all the videos in video_list}\par
2511 {\cf22 }\par
2512 {\cf22         key_index is the index for which video is the source that all others will use as alignment reference.}\par
2513 {\cf22 }\par
2514 {\cf22         returns a stitched image that is overlapped by all videos and a list of homography transformation matrices for every video index}\par
2515 {\cf22         '''}\par
2516 \par
2517         {\cf20 # Undistort all the rooms}\par
2518         room_images = []\par
2519         room_tracks = []\par
2520         {\cf19 for} video {\cf19 in} video_list:\par
2521             print(video)\par
2522             frame = self.collect_frames(video,1,1,2)[0]\par
2523             {\cf19 if} undistort:\par
2524                 frame = self.undistort_room(frame)\par
2525             room_images.append(frame)\par
2526         \par
2527         {\cf20 #}\par
2528         image_dict = \{\}\par
2529         orb = cv2.ORB_create()\par
2530         sift = cv2.SIFT_create()\par
2531 \par
2532         {\cf20 # Self defined keypoints with corner detection. }\par
2533         {\cf19 for} index, img {\cf19 in} enumerate(room_images):\par
2534             maxCorners = max(0, 200)\par
2535             {\cf20 # Parameters for Shi-Tomasi algorithm}\par
2536             qualityLevel = 0.01\par
2537             minDistance = 10\par
2538             blockSize = 3\par
2539             gradientSize = 3\par
2540             useHarrisDetector = {\cf17 False}\par
2541             k = 0.03\par
2542             {\cf20 # Copy the source image}\par
2543             copy = np.copy(img)\par
2544             gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\par
2545             {\cf20 # Apply corner detection}\par
2546             corners = cv2.goodFeaturesToTrack(gray, maxCorners, qualityLevel, minDistance, {\cf18 None}, blockSize=blockSize, gradientSize=gradientSize, useHarrisDetector=useHarrisDetector, k=k)\par
2547             {\cf20 # Draw corners detected}\par
2548             print({\cf22 '** Number of corners detected:'}, corners.shape[0])\par
2549             radius = 4\par
2550             {\cf19 for} i {\cf19 in} range(corners.shape[0]):\par
2551                 cv2.circle(copy, (int(corners[i,0,0]), int(corners[i,0,1])), radius, (0, 255, 0), cv2.FILLED)\par
2552             {\cf20 # Show what you got}\par
2553             {\cf19 if} show:\par
2554                 cv2.namedWindow({\cf22 "source_window"})\par
2555                 cv2.imshow({\cf22 "source_window"}, copy)\par
2556                 cv2.waitKey(600)\par
2557             {\cf20 # Set the needed parameters to find the refined corners}\par
2558             winSize = (5, 5)\par
2559             zeroZone = (-1, -1)\par
2560             criteria = (cv2.TERM_CRITERIA_EPS + cv2.TermCriteria_COUNT, 40, 0.001)\par
2561             {\cf20 # Calculate the refined corner locations}\par
2562             corners = cv2.cornerSubPix(gray, corners, winSize, zeroZone, criteria)\par
2563 \par
2564             {\cf20 # Convert subpixel corners to keypoints and create SIFT descriptors}\par
2565             kp_corners = cv2.KeyPoint_convert(corners)\par
2566             kp_corners, kp_des = sift.compute(img, kp_corners)\par
2567             img=cv2.drawKeypoints(img,kp_corners,img)\par
2568 \par
2569             {\cf20 # Store for later use}\par
2570             image_dict[index] = (kp_corners, kp_des, img, corners)\par
2571 \par
2572 \par
2573 \par
2574         matcher = cv2.BFMatcher()\par
2575         {\cf20 # matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_FLANNBASED)}\par
2576         {\cf20 # matcher = flann.knnMatch(des1,des2,k=2)}\par
2577         {\cf20 # FLANN_INDEX_KDTREE = 1}\par
2578         {\cf20 # index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 8)}\par
2579         {\cf20 # search_params = dict(checks = 100)}\par
2580         {\cf20 # flann = cv2.FlannBasedMatcher(index_params, search_params)}\par
2581         \par
2582         print({\cf22 "MATCHING"})\par
2583         {\cf19 if} key_index {\cf19 is} {\cf18 None}:\par
2584             source_images = image_dict.keys()\par
2585         {\cf19 else}:\par
2586             source_images = [key_index]\par
2587 \par
2588         accumulate = image_dict[source_images[0]][2]\par
2589         homographies = []\par
2590         {\cf19 for} img_key_1 {\cf19 in} source_images:\par
2591             accumulate = image_dict[img_key_1][2]\par
2592             {\cf19 for} img_key_2 {\cf19 in} image_dict.keys():\par
2593 \par
2594                 {\cf19 if} img_key_1 == img_key_2:\par
2595                     {\cf19 continue}\par
2596 \par
2597                 print(image_dict[img_key_1][3].shape)\par
2598                 d1 = image_dict[img_key_1][1]\par
2599                 d2 = image_dict[img_key_2][1]\par
2600 \par
2601                 kp1 = image_dict[img_key_1][0]\par
2602                 kp2 = image_dict[img_key_2][0]\par
2603                 img = image_dict[img_key_2][2]\par
2604                 matches = matcher.knnMatch(d1,d2, k=2)\par
2605                 {\cf20 # matches = flann.knnMatch(d1,d2, k=2)}\par
2606                 {\cf19 for} m {\cf19 in} matches:\par
2607                     print(m[0].distance)\par
2608 \par
2609                 good = []\par
2610                 {\cf19 for} m,n {\cf19 in} matches:\par
2611                     {\cf19 if} m.distance < 0.4*n.distance:\par
2612                         good.append(m)\par
2613                 \par
2614 \par
2615                 dst_pts = np.float32([ kp1[m.queryIdx].pt {\cf19 for} m {\cf19 in} good ]).reshape(-1,1,2)\par
2616                 src_pts = np.float32([ kp2[m.trainIdx].pt {\cf19 for} m {\cf19 in} good ]).reshape(-1,1,2)\par
2617                 \par
2618                 \par
2619                 {\cf20 # Sort them in the order of their distance.}\par
2620                 matches = sorted(matches, key = {\cf17 lambda} x:x[0].distance)\par
2621 \par
2622                 {\cf20 # print(matches)}\par
2623                 {\cf20 # img3 = cv2.drawMatchesKnn(image_dict[img_key_1][2], image_dict[img_key_1][0],}\par
2624                 {\cf20 #                         image_dict[img_key_2][2], image_dict[img_key_2][0],}\par
2625                 {\cf20 #                         matches[:10],}\par
2626                 {\cf20 #                         None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)}\par
2627                 {\cf20 # cv2.imshow("homography", img3)}\par
2628                 \par
2629 \par
2630 \par
2631                 {\cf20 # RHO = [Bazargani15]. (weighted RANSAC modification, faster in the case of many outliers)}\par
2632                 M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RHO, 10.0)\par
2633                 homographies.append((M,mask))\par
2634                 h,w, _ = img.shape\par
2635                 result = cv2.warpPerspective(image_dict[img_key_2][2], M,(w, h))\par
2636                 print(h)\par
2637 \par
2638                 accumulate = cv2.addWeighted(accumulate,0.8,result,0.4,0)\par
2639 \par
2640         {\cf20 # cv2.imshow("warped", accumulate)}\par
2641         {\cf20 # cv2.waitKey(0)}\par
2642         {\cf19 return} accumulate, homographies\par
2643 \par
}
{
\ql
References {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAAT" }{}}{\fldrslt {\cs37\ul\cf2 CameraPosition.collect_frames()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARR" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.collect_frames()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAART" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.undistort_room()}}}
.}\par
}
{\xe \v stitch_trackers\:room_estimation}
{\xe \v room_estimation\:stitch_trackers}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
stitch_trackers (  {\i self},   {\i video_list},   {\i homography_list},   {\i use_csv} = {\f2 True},   {\i offsets} = {\f2 [0,0,0,0]})}}
\par
{\bkmkstart AAAAAAAABS}
{\bkmkend AAAAAAAABS}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid returns a list of trackers\par
# offset = [top, left, bottom, right] in pixels\par
\par
We correct offsets if top and left edges are extended\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2644     {\cf17 def }stitch_trackers(self, video_list, homography_list, use_csv=True, offsets=[0,0,0,0]):\par
2645         {\cf22 '''}\par
2646 {\cf22         returns a list of trackers}\par
2647 {\cf22         # offset = [top, left, bottom, right] in pixels}\par
2648 {\cf22 }\par
2649 {\cf22         We correct offsets if top and left edges are extended}\par
2650 {\cf22         '''}\par
2651 \par
2652         dataframe_list = []\par
2653         {\cf19 for} index, video {\cf19 in} enumerate(video_list):\par
2654             data_path = str(video[:-3] + {\cf22 'csv'})\par
2655             image = self.collect_frames(video,1,1,2)[0]\par
2656             {\cf20 # Load data}\par
2657             {\cf19 try}:\par
2658                 data = self.load_tracker(data_path)\par
2659             {\cf19 except} Exception {\cf17 as} e:\par
2660                 print(e)\par
2661                 print({\cf22 "Skipping Data"})\par
2662                 {\cf19 continue}\par
2663 \par
2664             offset_top = offsets[0]\par
2665             offset_left = offsets[1]\par
2666             \par
2667             {\cf20 # Undistort data}\par
2668             data = self.undistort_tracker_data(data, image)\par
2669             x = data[{\cf22 'undistorted_x'}].to_numpy() + offset_left\par
2670             y = data[{\cf22 'undistorted_y'}].to_numpy() + offset_top\par
2671             z = [1] * len(x)\par
2672             points = np.vstack((x, y)).T\par
2673             points = points.reshape((-1, 1, 2))\par
2674             {\cf20 # points = points.reshape((1,3, len(x)))}\par
2675 \par
2676             {\cf20 # No transformation if we have the same image}\par
2677             {\cf19 if} index == 0:\par
2678                 print({\cf22 "NO DISTORTION"})\par
2679                 Matrix = np.identity(3)\par
2680                 Maks = {\cf18 None}\par
2681             {\cf19 else}:\par
2682                 Matrix, Mask = homography_list[index-1]\par
2683             {\cf20 # a_transformed = np.dot(Matrix, points)}\par
2684             {\cf20 # print(a_transformed)}\par
2685             {\cf20 # Note homography_lost[index] = (TransformationMatrix, Mask)}\par
2686             {\cf20 # Project points with stitched homographies}\par
2687             projected_points = cv2.perspectiveTransform(points, Matrix).T\par
2688 \par
2689 \par
2690             new_x = projected_points[0][0]\par
2691             new_y = projected_points[1][0]\par
2692 \par
2693             data[{\cf22 'stitched_x'}] = new_x\par
2694             data[{\cf22 'stitched_y'}] = new_y\par
2695 \par
2696             print(data)\par
2697 \par
2698             dataframe_list.append(data)\par
2699             \par
2700 \par
2701         {\cf19 return} dataframe_list\par
2702 \par
2703 \par
}
{
\ql
References {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAAT" }{}}{\fldrslt {\cs37\ul\cf2 CameraPosition.collect_frames()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARR" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.collect_frames()}}}
, {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAARV" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.load_tracker()}}}
, and room_estimation.undistort_tracker_data().}\par
}
{\xe \v superimpose_checker\:room_estimation}
{\xe \v room_estimation\:superimpose_checker}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
superimpose_checker (  {\i self},   {\i size} = {\f2 21})}}
\par
{\bkmkstart AAAAAAAARJ}
{\bkmkend AAAAAAAARJ}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid This draws the grid from "define_perspective_grid" onto an image. \par
\par
This grid is given a "size" that is the number of lines present. \par
This function utilizes draw_grid which is a 1080x1080 resolution image with cv2.lines drawn on it.\par
\par
This grid image is then registered onto defined points from "define_perspective_grid" given the homograpy.\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1529     {\cf17 def }superimpose_checker(self, size=21):\par
1530         {\cf22 '''}\par
1531 {\cf22         This draws the grid from "define_perspective_grid" onto an image. }\par
1532 {\cf22 }\par
1533 {\cf22         This grid is given a "size" that is the number of lines present. }\par
1534 {\cf22         This function utilizes draw_grid which is a 1080x1080 resolution image with cv2.lines drawn on it.}\par
1535 {\cf22 }\par
1536 {\cf22         This grid image is then registered onto defined points from "define_perspective_grid" given the homograpy.}\par
1537 {\cf22         '''}\par
1538         \par
1539         temp = deepcopy(self.image)\par
1540         {\cf20 # grid = cv2.imread("K:/Github/PeopleTracker/Grid.jpg")}\par
1541         print({\cf22 "\\n \\n Creating Grid..."})\par
1542         corners = self.get_room_corners()\par
1543 \par
1544         grid = self.draw_grid()\par
1545         h, w, _ = grid.shape\par
1546         src = np.array(((0,0), (w-1,0), (w-1,h-1), (0,h-1)))\par
1547 \par
1548         h, status = cv2.findHomography(src, corners)\par
1549         self.clear_corners()\par
1550         print({\cf22 "+/- to change grid, Q to quit."})\par
1551         {\cf19 while} {\cf17 True}:\par
1552             grid = self.draw_grid(grid_shape=(size,size))\par
1553             registered = cv2.warpPerspective(grid, h, (temp.shape[1], temp.shape[0]))\par
1554             registered = cv2.addWeighted(temp,1,registered,0.3,0)\par
1555             cv2.imshow({\cf22 "Registered"}, registered)\par
1556             k = cv2.waitKey(1)\par
1557             {\cf19 if} k == ord({\cf22 "="}):\par
1558                 size += 1\par
1559             {\cf19 if} k == ord({\cf22 '-'}):\par
1560                 size -= 1\par
1561             {\cf19 if} k == ord({\cf22 'q'}):\par
1562                 {\cf19 break}\par
1563 \par
1564             \par
1565 \par
1566         {\cf19 return} registered\par
1567 \par
}
{
\ql
References room_estimation.clear_corners(), room_estimation.draw_grid(), {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAAQD" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.get_room_corners()}}}
, App.image, room_estimation.image, and person_tab.image.}\par
}
{\xe \v undistort_room\:room_estimation}
{\xe \v room_estimation\:undistort_room}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
undistort_room (  {\i self},   {\i image} = {\f2 None},   {\i points_data} = {\f2 None},   {\i camera_matrix} = {\f2 None},   {\i distortion} = {\f2 None},   {\i show} = {\f2 False},   {\i fisheye} = {\f2 False},   {\i calibration_resolution} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAART}
{\bkmkend AAAAAAAART}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid }
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1953     {\cf17 def }undistort_room(self, image=None, points_data=None, camera_matrix=None, distortion=None, show=False, fisheye=False, calibration_resolution=None):\par
1954         {\cf22 '''}\par
1955 {\cf22         '''}\par
1956         {\cf19 if} camera_matrix {\cf19 is} {\cf18 None}:\par
1957             camera_matrix = self.camera_matrix\par
1958         {\cf19 if} distortion {\cf19 is} {\cf18 None}:\par
1959             distortion = self.distioriton_matrix\par
1960 \par
1961         {\cf19 if} image {\cf19 is} {\cf18 None}: \par
1962             image = self.image\par
1963         \par
1964         {\cf19 if} calibration_resolution {\cf19 is} {\cf18 None}:\par
1965             calibration_resolution = self.calibration_resolution\par
1966         \par
1967         width = image.shape[1]\par
1968         height = image.shape[0]\par
1969 \par
1970         \par
1971         {\cf19 if} fisheye:\par
1972             img_dim = image.shape[:2][::-1]  \par
1973             scaled_K = camera_matrix * img_dim[0] / width\par
1974             scaled_K[2][2] = 1.0  \par
1975 \par
1976             {\cf22 '''}\par
1977 {\cf22             new_K = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(scaled_K, distortion,}\par
1978 {\cf22                                                                         img_dim, np.eye(3), balance=0, fov_scale=1)}\par
1979 {\cf22 }\par
1980 {\cf22             map1, map2 = cv2.fisheye.initUndistortRectifyMap(scaled_K, distortion, np.eye(3),}\par
1981 {\cf22                                                                     new_K, img_dim, cv2.CV_16SC2)}\par
1982 {\cf22             undistorted_image = cv2.remap(image, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)}\par
1983 {\cf22             '''}\par
1984 \par
1985             {\cf22 '''}\par
1986 {\cf22             # New Camera matrix is }\par
1987 {\cf22             newcameramatrix = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(camera_matrix, distortion, img_dim, np.eye(3), balance=0, fov_scale=1)}\par
1988 {\cf22             undistorted_image = cv2.fisheye.undistortImage(image, camera_matrix, distortion, None, newcameramatrix)         }\par
1989 {\cf22             '''}             \par
1990             {\cf20 # '''}\par
1991             map1, map2 = cv2.fisheye.initUndistortRectifyMap(camera_matrix, distortion, np.eye(3), camera_matrix, (width,height), cv2.CV_16SC2)\par
1992             undistorted_image = cv2.remap(image, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\par
1993             {\cf20 # '''}\par
1994         {\cf19 else}:\par
1995             {\cf20 # self.image.shape[1]}\par
1996             {\cf20 # h1, w1, _ = image.shape}\par
1997             {\cf20 # #Account for resolution differences between calibration and image input}\par
1998             {\cf20 # w_ratio, h_ratio = self.get_calibration_ratio((w1,h1), calibration_resolution)}\par
1999             {\cf20 # print(w_ratio, h_ratio)}\par
2000 \par
2001             {\cf20 # camera_matrix[0][0] = camera_matrix[0][0] * w_ratio}\par
2002             {\cf20 # camera_matrix[1][1] = camera_matrix[1][1] * h_ratio}\par
2003             {\cf20 # camera_matrix[0][2] = camera_matrix[0][2] * w_ratio}\par
2004             {\cf20 # camera_matrix[1][2] = camera_matrix[1][2] * h_ratio}\par
2005 \par
2006             {\cf20 # for index, d in enumerate(distortion[0]):}\par
2007             {\cf20 #     distortion[0][index] = [0][index] * w_ratio}\par
2008             \par
2009             newcameramatrix, _ = cv2.getOptimalNewCameraMatrix(camera_matrix, distortion, (width, height), 1, (width, height))\par
2010             {\cf19 if} points_data {\cf19 is} {\cf19 not} {\cf18 None}:\par
2011 \par
2012 \par
2013                 {\cf20 #NOTE Solution to this being wrong, we should just calibrate the camera in the same resolution as our videos}\par
2014                 x = data[{\cf22 'Pixel_Loc_x'}].to_numpy()\par
2015                 y = data[{\cf22 'Pixel_Loc_y'}].to_numpy()\par
2016                 points = np.vstack((y, x)).T\par
2017 \par
2018                 {\cf20 # d2 = distortion}\par
2019                 {\cf20 # for index, d in enumerate(distortion[0]):}\par
2020                 {\cf20 # d2 = distortion[0] * w_ratio}\par
2021 \par
2022                 pts_r_norm = cv2.undistortPoints(points, camera_matrix, distortion, {\cf18 None}, newcameramatrix)\par
2023 \par
2024                 new_x = pts_r_norm.T[1]\par
2025                 new_y = pts_r_norm.T[0]\par
2026 \par
2027                 data[{\cf22 'undistorted_x'}] = new_x[0].tolist()\par
2028                 data[{\cf22 'undistorted_y'}] = new_y[0].tolist()\par
2029                 self.points_data = data\par
2030             undistorted_image = cv2.undistort(image, camera_matrix, distortion, {\cf18 None}, newcameramatrix)\par
2031 \par
2032 \par
2033 \par
2034 \par
2035             {\cf20 # if show:}\par
2036             {\cf20 #     cv2.imshow("undistorted", undistorted_image)}\par
2037             {\cf20 #     cv2.waitKey(0)}\par
2038 \par
2039         {\cf19 if} show:\par
2040             cv2.imshow({\cf22 "undistorted"}, undistorted_image)\par
2041             cv2.waitKey(0)\par
2042         {\cf19 return} undistorted_image\par
2043 \par
}
{
\ql
References room_estimation.calibration_resolution, room_estimation.camera_matrix, room_estimation.distioriton_matrix, App.image, room_estimation.image, person_tab.image, and room_estimation.points_data.}\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABQ" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.save_room()}}}
, and {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABR" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.stitch_rooms()}}}
.}\par
}
{\xe \v undistort_tracker_data\:room_estimation}
{\xe \v room_estimation\:undistort_tracker_data}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
undistort_tracker_data (  {\i self},   {\i data},   {\i image} = {\f2 None},   {\i camera_matrix} = {\f2 None},   {\i distortion} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAARU}
{\bkmkend AAAAAAAARU}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2044     {\cf17 def }undistort_tracker_data(self,data, image=None, camera_matrix=None, distortion=None):\par
2045         {\cf19 if} camera_matrix {\cf19 is} {\cf18 None}:\par
2046             camera_matrix = self.camera_matrix\par
2047         {\cf19 if} distortion {\cf19 is} {\cf18 None}:\par
2048             distortion = self.distioriton_matrix\par
2049         \par
2050         {\cf19 if} image {\cf19 is} {\cf18 None}:\par
2051             image = self.image\par
2052 \par
2053         {\cf20 #Image dimensions}\par
2054         width = image.shape[1]\par
2055         height = image.shape[0]\par
2056 \par
2057 \par
2058         x = data[{\cf22 'Pixel_Loc_x'}].to_numpy()\par
2059         y = data[{\cf22 'Pixel_Loc_y'}].to_numpy()\par
2060 \par
2061         print((width, height), self.calibration_resolution)\par
2062 \par
2063 \par
2064         points = np.vstack((y, x)).T\par
2065         \par
2066         {\cf20 # h1, w1, _ = image.shape}\par
2067         {\cf20 # # Account for resolution differences between calibration and image input}\par
2068         {\cf20 # w_ratio, h_ratio = self.get_calibration_ratio((w1,h1), self.calibration_resolution)}\par
2069         {\cf20 # print(w_ratio, h_ratio)}\par
2070 \par
2071         {\cf20 # camera_matrix[0][0] = camera_matrix[0][0] * w_ratio}\par
2072         {\cf20 # camera_matrix[1][1] = camera_matrix[1][1] * h_ratio}\par
2073         {\cf20 # camera_matrix[0][2] = camera_matrix[0][2] * w_ratio}\par
2074         {\cf20 # camera_matrix[1][2] = camera_matrix[1][2] * h_ratio}\par
2075 \par
2076         newcameramatrix, _ = cv2.getOptimalNewCameraMatrix(camera_matrix, distortion, (width, height), 1, (width, height))\par
2077         {\cf20 # numpy_points = np.reshape(np.asfarray(points), (len(points),2,1))}\par
2078 \par
2079 \par
2080 \par
2081         pts_r_norm = cv2.undistortPoints(points, camera_matrix, distortion, {\cf18 None}, newcameramatrix)\par
2082 \par
2083         new_x = pts_r_norm.T[1]\par
2084         new_y = pts_r_norm.T[0]\par
2085 \par
2086 \par
2087         data[{\cf22 'undistorted_x'}] = new_x[0].tolist()\par
2088         data[{\cf22 'undistorted_y'}] = new_y[0].tolist()\par
2089 \par
2090         {\cf20 # for i in range(new_x.shape[0]):}\par
2091         {\cf20 #     # print('showing')}\par
2092         {\cf20 #     p = (int(new_x[0][i]),int(new_y[0][i]))}\par
2093         {\cf20 #     cv2.circle(self.show_img,p,1,color=(0,255,255), thickness=4)}\par
2094         {\cf20 # cv2.imshow("Projected", self.show_img)}\par
2095         {\cf20 # cv2.waitKey(0)}\par
2096 \par
2097         {\cf19 return} data\par
2098 \par
2099 \par
}
\par
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABS" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.stitch_trackers()}}}
.}\par
}
{\xe \v update_tracker_3D\:room_estimation}
{\xe \v room_estimation\:update_tracker_3D}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
update_tracker_3D (  {\i self},   {\i index},   {\i df},   {\i mapped_min},   {\i mapped_max})}}
\par
{\bkmkstart AAAAAAAARX}
{\bkmkend AAAAAAAARX}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid Updates the already created plot with new data\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2140     {\cf17 def }update_tracker_3D(self,index, df, mapped_min, mapped_max):\par
2141         {\cf22 '''}\par
2142 {\cf22         Updates the already created plot with new data}\par
2143 {\cf22         '''}\par
2144 \par
}
}
{\xe \v visualize_distortion\:room_estimation}
{\xe \v room_estimation\:visualize_distortion}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
visualize_distortion (  {\i self},   {\i camera_matrix},   {\i distiortion},   {\i image} = {\f2 None})}}
\par
{\bkmkstart AAAAAAAASD}
{\bkmkend AAAAAAAASD}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{
\par
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#void%20projectPoints(InputArray%20objectPoints,%20InputArray%20rvec,%20InputArray%20tvec,%20InputArray%20cameraMatrix,%20InputArray%20distCoeffs,%20OutputArray%20imagePoints,%20OutputArray%20jacobian,%20double%20aspectRatio)\par
\par
opencv 3 here: \par
https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html\par
\par
[x] =  [X]\par
[y] = R[Y] + t\par
[z] =  [Z]\par
\par
x' = x/z\par
y' = y/z\par
\par
NOTE: k1-6 are radial distortion coefficients, p1 and p2 are tangential distortion\par
        if any \par
\par
x'' = x' (1 + k1 * r^2+k2 + k3*r^4 + k3*r^6)/ + 2*p1*x'*y' + p2(r^2 + (2x')^2)\par
         (1 + k4 * r^2+k5 + k3*r^4 + k6*r^6)\par
\par
y'' = y' (1 + k1 * r^2+k2 + k3*r^4 + k3*r^6)/ + p1(r^2 + (2y')^2) + 2*p2 * x' * y'\par
         (1 + k4 * r^2+k5 + k3*r^4 + k6*r^6)\par
\par
r^2 = x'^2 + y'^2\par
u = fx * x'' + cx\par
v = fy * y'' + cy\par
}
 \par
}{
\pard\plain \s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 2327     {\cf17 def }visualize_distortion(self, camera_matrix, distiortion, image=None):\par
2328         {\cf22 '''}\par
2329 {\cf22         https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#void%20projectPoints(InputArray%20objectPoints,%20InputArray%20rvec,%20InputArray%20tvec,%20InputArray%20cameraMatrix,%20InputArray%20distCoeffs,%20OutputArray%20imagePoints,%20OutputArray%20jacobian,%20double%20aspectRatio)}\par
2330 {\cf22 }\par
2331 {\cf22         opencv 3 here: }\par
2332 {\cf22         https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html}\par
2333 {\cf22 }\par
2334 {\cf22         [x] =  [X]}\par
2335 {\cf22         [y] = R[Y] + t}\par
2336 {\cf22         [z] =  [Z]}\par
2337 {\cf22 }\par
2338 {\cf22         x' = x/z}\par
2339 {\cf22         y' = y/z}\par
2340 {\cf22 }\par
2341 {\cf22         NOTE: k1-6 are radial distortion coefficients, p1 and p2 are tangential distortion}\par
2342 {\cf22                 if any }\par
2343 {\cf22 }\par
2344 {\cf22         x'' = x' (1 + k1 * r^2+k2 + k3*r^4 + k3*r^6)/ + 2*p1*x'*y' + p2(r^2 + (2x')^2)}\par
2345 {\cf22                  (1 + k4 * r^2+k5 + k3*r^4 + k6*r^6)}\par
2346 {\cf22 }\par
2347 {\cf22         y'' = y' (1 + k1 * r^2+k2 + k3*r^4 + k3*r^6)/ + p1(r^2 + (2y')^2) + 2*p2 * x' * y'}\par
2348 {\cf22                  (1 + k4 * r^2+k5 + k3*r^4 + k6*r^6)}\par
2349 {\cf22 }\par
2350 {\cf22         r^2 = x'^2 + y'^2}\par
2351 {\cf22         u = fx * x'' + cx}\par
2352 {\cf22         v = fy * y'' + cy}\par
2353 {\cf22         '''}\par
2354         {\cf19 if} image {\cf19 is} {\cf18 None}:\par
2355             image = self.image\par
2356         \par
2357         height, width, _ = image.shape\par
2358         nstep = 20\par
2359         d = list(distiortion[0])\par
2360         {\cf20 # print(d)}\par
2361         k1, k2, p1, p2, k3, k4, k5, k6, s1, s2, s3, s4, taux, tauy = list(distiortion[0])\par
2362         fx = camera_matrix[0,0]\par
2363         fy = camera_matrix[1,1]\par
2364         cx = camera_matrix[0,2]\par
2365         cy = camera_matrix[1,2]\par
2366 \par
2367         {\cf20 # Create a grid from the size of the image}\par
2368         uv = np.meshgrid(np.linspace(-width,width-1,nstep), np.linspace(-height,height-1,nstep))\par
2369         {\cf20 # print(uv)}\par
2370         {\cf20 # NOTE: p stands for prime (')}\par
2371         u = uv[0]\par
2372         v = uv[1]\par
2373         z = np.ones(u.shape)\par
2374         xp = u / z {\cf20 # z=1}\par
2375         yp = v / z {\cf20 # z=1}\par
2376 \par
2377         {\cf20 # print(xp, yp)}\par
2378 \par
2379         r2 = xp ** 2 + yp ** 2\par
2380         r4 = r2 ** 2 \par
2381         r6 = r2 ** 3\par
2382         coef = (1 + k1*r2 + k2*r4 + k3*r6)/(1 + k4*r2 + k5*r4 + k6*r6)\par
2383         \par
2384         {\cf20 # print(coef)}\par
2385 \par
2386         xpp = xp * coef + 2*p1*(xp*yp) + p2*(r2 + 2*xp**2) + s1*r2 + s2*r4\par
2387         ypp = yp * coef + p1*(r2 + 2*yp**2) + 2 * p2 * (xp * yp) + s1*r2 + s2*r4\par
2388 \par
2389         x_radial_distortion = xp * (1 + k1*r2 + k2*r4 + k3*r6) {\cf20 # xp}\par
2390         y_radial_distortion = yp * (1 + k1*r2 + k2*r4 + k3*r6) {\cf20 # yp}\par
2391 \par
2392         x_tangential_distortion = xp + (2*p1*xp*yp +p2*(r2 + 2*xp**2)) {\cf20 # xpp}\par
2393         y_tangential_distortion = yp + (p1*(r2 + 2*yp**2) + 2 * p2 * xp * yp) {\cf20 # ypp}\par
2394 \par
2395 \par
2396         u2 = fx * xpp + cx\par
2397         v2 = fy * ypp + cy\par
2398 \par
2399         difference_u = u2[:] - u[:]\par
2400         difference_v = v2[:] - v[:]\par
2401 \par
2402         {\cf20 # du, dv = np.meshgrid(difference_u, difference_v)}\par
2403         {\cf20 # dr = np.reshape(np.hypot(difference_u,difference_v), xp.shape)}\par
2404         dr = np.reshape(np.hypot(difference_u,difference_v), u.shape)\par
2405         {\cf20 # print(dr)}\par
2406         center = (width/2, height/2)\par
2407         principal_point = (cx, cy)\par
2408 \par
2409         fix, ax = plt.subplots()\par
2410         ax.scatter(center[0], center[1], marker={\cf22 "o"})\par
2411         ax.scatter(principal_point[0], principal_point[1], marker={\cf22 "X"})\par
2412         CS = ax.contour(u[1,:]+1, v[:,1]+1, dr)\par
2413         {\cf20 # ax.quiver(u[:]+1, v[:]+1, du, dv)}\par
2414         ax.clabel(CS, fontsize=9, inline={\cf17 True})\par
2415         plt.show()\par
2416         {\cf20 # dr = reshape(hypot(du,dv), size(u));}\par
2417 \par
}
{
\ql
Referenced by {\field {\*\fldinst { HYPERLINK  \\l "AAAAAAAABQ" }{}}{\fldrslt {\cs37\ul\cf2 room_estimation.save_room()}}}
.}\par
}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
The documentation for this class was generated from the following file:{\par
\pard\plain \s121\fi-360\li720\widctlpar\jclisttab\tx720{\*\pn \pnlvlbody\ilvl0\ls2\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
K:/Github/PeopleTracker/src/room_estimation.py\par
}}